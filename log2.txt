Starting LoRA Finetuning...
Output directory: ./output/finetune_lora
[2025-07-04 21:01:54,605] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:01:57,161] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=1: setting --include=localhost:1
[2025-07-04 21:01:57,162] [INFO] [runner.py:605:main] cmd = /data/macaulay/envs/protein2text_env/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ./scripts/finetune.py --output_dir ./output/finetune_lora --deepspeed ./configs/zero2.json --model_config_file ./configs/lora_config.yaml
[2025-07-04 21:01:58,547] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:02:01,062] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [1]}
[2025-07-04 21:02:01,062] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-07-04 21:02:01,062] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-07-04 21:02:01,062] [INFO] [launch.py:164:main] dist_world_size=1
[2025-07-04 21:02:01,062] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=1
[2025-07-04 21:02:01,063] [INFO] [launch.py:256:main] process 519873 spawned with command: ['/data/macaulay/envs/protein2text_env/bin/python3.10', '-u', './scripts/finetune.py', '--local_rank=0', '--output_dir', './output/finetune_lora', '--deepspeed', './configs/zero2.json', '--model_config_file', './configs/lora_config.yaml']
[2025-07-04 21:02:03,351] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:02:05,687] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-07-04 21:02:05,687] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading configuration from YAML: ./configs/lora_config.yaml
Effective Model Arguments: {'model_name_or_path': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'protein_encoder_name_or_path': 'esm3_sm_open_v1', 'mm_resampler': True, 'mm_gated_cross_attention': True, 'mm_projector_type': 'mlp2x_gelu', 'num_media_tokens': 16, 'mm_protein_select_layer': -2, 'use_mm_proj': True, 'pretrained_adapter_path': './output/pretrain_gca/', 'lora_enable': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'lora_target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_bias': 'none', 'tune_mm_mlp_adapter': True, 'esm_hidden_size': 1536, 'dim_head': 64, 'ff_mult': 4, 'perceiver_depth': 6, 'gca_output_dim': 512, 'mm_gca_num_heads': 8, 'mm_resampler_num_heads': 8, 'resampler_output_dim': 1536}
Effective Data Arguments: {'data_path': './data/test.json', 'eval_data_path': '/data/macaulay/Mutation2Text/data/finetune_eval_data.json', 'require_both_sequences': True, 'max_text_len': 1536}
Effective Training Arguments: {'output_dir': './output/finetune_lora', 'overwrite_output_dir': False, 'do_train': True, 'do_eval': True, 'do_predict': False, 'evaluation_strategy': <IntervalStrategy.STEPS: 'steps'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 1e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 5, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.03, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': './output/finetune_lora/runs/Jul04_21-02-05_avi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': <IntervalStrategy.STEPS: 'steps'>, 'save_steps': 4000, 'save_total_limit': 2, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': True, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 200, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': './output/finetune_lora', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'deepspeed': './configs/zero2.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'mode': 'train'}
Effective Training Arguments (deepspeed config): ./configs/zero2.json
Warning: DeepSpeed is enabled with ZeRO stage 2, which is not Stage 3. Evaluation with DeepSpeed ZeRO inference requires Stage 3. Disabling evaluation for this run. To enable, use a ZeRO Stage 3 config or set evaluation_strategy to 'no'.
[DEBUG] Training arguments: CustomTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./configs/zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=200,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./output/finetune_lora/runs/Jul04_21-02-05_avi,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mode=train,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5,
optim=adamw_torch,
optim_args=None,
output_dir=./output/finetune_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./output/finetune_lora,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=4000,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
[DEBUG] Model arguments: ModelArguments(model_name_or_path='meta-llama/Meta-Llama-3.1-8B-Instruct', protein_encoder_name_or_path='esm3_sm_open_v1', mm_resampler=True, mm_gated_cross_attention=True, mm_projector_type='mlp2x_gelu', num_media_tokens=16, mm_protein_select_layer=-2, use_mm_proj=True, pretrained_adapter_path='./output/pretrain_gca/', lora_enable=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_bias='none', tune_mm_mlp_adapter=True, esm_hidden_size=1536, dim_head=64, ff_mult=4, perceiver_depth=6, gca_output_dim=512, mm_gca_num_heads=8, mm_resampler_num_heads=8, resampler_output_dim=1536)
[DEBUG] Data arguments: DataArguments(data_path='./data/test.json', eval_data_path='/data/macaulay/Mutation2Text/data/finetune_eval_data.json', require_both_sequences=True, max_text_len=1536)
[DEBUG] Loading model and tokenizer...
[DEBUG] Loading model and tokenizer.
[DEBUG] Setting pad_token to eos_token
[DEBUG] Padding token ID set to: 128009
[DEBUG] Adding special tokens: ['<wt_protein>', '</wt_protein>', '<mut_protein>', '</mut_protein>']
[DEBUG] WT protein start token ID: 128256
[DEBUG] WT protein end token ID: 128257
[DEBUG] Mut protein start token ID: 128258
[DEBUG] Mut protein end token ID: 128259
Config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "mut_protein_end_token_id": 128259,
  "mut_protein_start_token_id": 128258,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128009,
  "pretraining_tp": 1,
  "protein_config": {
    "esm_hidden_size": 1536,
    "gca_output_dim": 512,
    "mm_gated_cross_attention": true,
    "mm_gca_num_heads": 8,
    "mm_projector_type": "mlp2x_gelu",
    "mm_protein_select_layer": -2,
    "mm_resampler": true,
    "mm_resampler_num_heads": 8,
    "num_media_tokens": 16,
    "protein_encoder_name_or_path": "esm3_sm_open_v1",
    "resampler_output_dim": 1536,
    "use_mm_proj": true
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "tune_mm_mlp_adapter": true,
  "use_cache": true,
  "vocab_size": 128256,
  "wt_protein_end_token_id": 128257,
  "wt_protein_start_token_id": 128256
}

[DEBUG] Configuring protein encoder.
[DEBUG] mode detected: train
Loading ESM3 client: esm3_sm_open_v1 to device: cpu
ESM3 client loaded successfully. Deduced output embedding dimension: 1536
[DEBUG] Detected ESM3 output embedding dim: 1536
[DEBUG] Initializing LlavaLlamaForCausalLM with protein_config: {'protein_encoder_name_or_path': 'esm3_sm_open_v1', 'mm_gated_cross_attention': True, 'mm_resampler': True, 'num_media_tokens': 16, 'mm_projector_type': 'mlp2x_gelu', 'use_mm_proj': True, 'mm_protein_select_layer': -2, 'esm_hidden_size': 1536, 'gca_output_dim': 512, 'resampler_output_dim': 1536, 'mm_gca_num_heads': 8, 'mm_resampler_num_heads': 8}
[DEBUG] Initializing GCA module with parameters:
  esm_hidden_size: 1536
  gca_output_dim: 512
  num_heads: 8
[DEBUG FF Init] Dimensions:
  input_dim: 512
  inner_dim: 2048
[DEBUG] GCA module initialized successfully
[DEBUG] Initializing Resampler module with parameters:
  gca_output_dim: 512
  resampler_output_dim: 1536
  num_media_tokens: 16
  num_heads: 8
[DEBUG FF Init] Dimensions:
  input_dim: 512
  inner_dim: 2048
[DEBUG FF Init] Dimensions:
  input_dim: 512
  inner_dim: 2048
[DEBUG] Initializing Projector module
[DEBUG] Projector configuration:
  type: mlp2x_gelu
  input_dim (from resampler): 1536
  output_dim (LLM hidden): 4096
[DEBUG] Initializing MLPProjector:
  input_dim: 1536
  hidden_dim: 3072
  output_dim: 4096
[DEBUG] MLPProjector parameter dtypes:
  linear1.weight: torch.float32
  linear1.bias: torch.float32
  linear2.weight: torch.float32
  linear2.bias: torch.float32
[DEBUG] Projector module initialized successfully with proper dimension matching
[DEBUG] Resizing token embeddings without mean resizing
[DEBUG] Initialized embedding for wt protein start token ID 128256
[DEBUG] Initialized embedding for wt protein end token ID 128257
[DEBUG] Initialized embedding for mut protein start token ID 128258
[DEBUG] Initialized embedding for mut protein end token ID 128259
Protein encoder frozen.
Protein encoder set to eval mode.
[Model Utils] Unfreezing GCA/Resampler/Projector
GCA unfrozen.
Resampler unfrozen.
Projector unfrozen.

[DEBUG] Module trainable status after unfreezing:
  mm_gated_cross_attention.ff_gate: requires_grad = True
  mm_gated_cross_attention.norm_query.weight: requires_grad = True
  mm_gated_cross_attention.norm_query.bias: requires_grad = True
  mm_gated_cross_attention.norm_key_value.weight: requires_grad = True
  mm_gated_cross_attention.norm_key_value.bias: requires_grad = True
  mm_gated_cross_attention.to_q_cross.weight: requires_grad = True
  mm_gated_cross_attention.to_kv_cross.weight: requires_grad = True
  mm_gated_cross_attention.to_qkv_self_attn.weight: requires_grad = True
  mm_gated_cross_attention.gate_norm.weight: requires_grad = True
  mm_gated_cross_attention.gate_norm.bias: requires_grad = True
  mm_gated_cross_attention.gate_attention.in_proj_weight: requires_grad = True
  mm_gated_cross_attention.gate_attention.out_proj.weight: requires_grad = True
  mm_gated_cross_attention.to_mixing_weights.0.weight: requires_grad = True
  mm_gated_cross_attention.to_mixing_weights.0.bias: requires_grad = True
  mm_gated_cross_attention.to_out.weight: requires_grad = True
  mm_gated_cross_attention.ff.norm.weight: requires_grad = True
  mm_gated_cross_attention.ff.norm.bias: requires_grad = True
  mm_gated_cross_attention.ff.fc1.weight: requires_grad = True
  mm_gated_cross_attention.ff.fc2.weight: requires_grad = True
  mm_resampler.latents: requires_grad = True
  mm_resampler.latent_pos_emb: requires_grad = True
  mm_resampler.layers.0.0.norm_media.weight: requires_grad = True
  mm_resampler.layers.0.0.norm_media.bias: requires_grad = True
  mm_resampler.layers.0.0.norm_latents.weight: requires_grad = True
  mm_resampler.layers.0.0.norm_latents.bias: requires_grad = True
  mm_resampler.layers.0.0.to_q.weight: requires_grad = True
  mm_resampler.layers.0.0.to_kv.weight: requires_grad = True
  mm_resampler.layers.0.0.to_out.weight: requires_grad = True
  mm_resampler.layers.0.1.norm.weight: requires_grad = True
  mm_resampler.layers.0.1.norm.bias: requires_grad = True
  mm_resampler.layers.0.1.fc1.weight: requires_grad = True
  mm_resampler.layers.0.1.fc2.weight: requires_grad = True
  mm_resampler.layers.1.0.norm_media.weight: requires_grad = True
  mm_resampler.layers.1.0.norm_media.bias: requires_grad = True
  mm_resampler.layers.1.0.norm_latents.weight: requires_grad = True
  mm_resampler.layers.1.0.norm_latents.bias: requires_grad = True
  mm_resampler.layers.1.0.to_q.weight: requires_grad = True
  mm_resampler.layers.1.0.to_kv.weight: requires_grad = True
  mm_resampler.layers.1.0.to_out.weight: requires_grad = True
  mm_resampler.layers.1.1.norm.weight: requires_grad = True
  mm_resampler.layers.1.1.norm.bias: requires_grad = True
  mm_resampler.layers.1.1.fc1.weight: requires_grad = True
  mm_resampler.layers.1.1.fc2.weight: requires_grad = True
  mm_resampler.norm.weight: requires_grad = True
  mm_resampler.norm.bias: requires_grad = True
  mm_resampler.proj_out.weight: requires_grad = True
  mm_resampler.proj_out.bias: requires_grad = True
  mm_projector.linear1.weight: requires_grad = True
  mm_projector.linear1.bias: requires_grad = True
  mm_projector.linear2.weight: requires_grad = True
  mm_projector.linear2.bias: requires_grad = True
Attempting to load pretrained adapter weights from directory: ./output/pretrain_gca/
Loading adapter weights from ./output/pretrain_gca/adapters_only.pt
Loading GCA weights...
Loading Resampler weights...
Loading Projector weights...
Successfully loaded adapter weights from ./output/pretrain_gca/adapters_only.pt.
[DEBUG] Training mode: train
[DEBUG] Configuring trainable modules.
LoRA finetuning mode
Trainable before PEFT: model.embed_tokens.weight
Trainable before PEFT: model.layers.0.self_attn.q_proj.weight
Trainable before PEFT: model.layers.0.self_attn.k_proj.weight
Trainable before PEFT: model.layers.0.self_attn.v_proj.weight
Trainable before PEFT: model.layers.0.self_attn.o_proj.weight
Trainable before PEFT: model.layers.0.mlp.gate_proj.weight
Trainable before PEFT: model.layers.0.mlp.up_proj.weight
Trainable before PEFT: model.layers.0.mlp.down_proj.weight
Trainable before PEFT: model.layers.0.input_layernorm.weight
Trainable before PEFT: model.layers.0.post_attention_layernorm.weight
Trainable before PEFT: model.layers.1.self_attn.q_proj.weight
Trainable before PEFT: model.layers.1.self_attn.k_proj.weight
Trainable before PEFT: model.layers.1.self_attn.v_proj.weight
Trainable before PEFT: model.layers.1.self_attn.o_proj.weight
Trainable before PEFT: model.layers.1.mlp.gate_proj.weight
Trainable before PEFT: model.layers.1.mlp.up_proj.weight
Trainable before PEFT: model.layers.1.mlp.down_proj.weight
Trainable before PEFT: model.layers.1.input_layernorm.weight
Trainable before PEFT: model.layers.1.post_attention_layernorm.weight
Trainable before PEFT: model.layers.2.self_attn.q_proj.weight
Trainable before PEFT: model.layers.2.self_attn.k_proj.weight
Trainable before PEFT: model.layers.2.self_attn.v_proj.weight
Trainable before PEFT: model.layers.2.self_attn.o_proj.weight
Trainable before PEFT: model.layers.2.mlp.gate_proj.weight
Trainable before PEFT: model.layers.2.mlp.up_proj.weight
Trainable before PEFT: model.layers.2.mlp.down_proj.weight
Trainable before PEFT: model.layers.2.input_layernorm.weight
Trainable before PEFT: model.layers.2.post_attention_layernorm.weight
Trainable before PEFT: model.layers.3.self_attn.q_proj.weight
Trainable before PEFT: model.layers.3.self_attn.k_proj.weight
Trainable before PEFT: model.layers.3.self_attn.v_proj.weight
Trainable before PEFT: model.layers.3.self_attn.o_proj.weight
Trainable before PEFT: model.layers.3.mlp.gate_proj.weight
Trainable before PEFT: model.layers.3.mlp.up_proj.weight
Trainable before PEFT: model.layers.3.mlp.down_proj.weight
Trainable before PEFT: model.layers.3.input_layernorm.weight
Trainable before PEFT: model.layers.3.post_attention_layernorm.weight
Trainable before PEFT: model.layers.4.self_attn.q_proj.weight
Trainable before PEFT: model.layers.4.self_attn.k_proj.weight
Trainable before PEFT: model.layers.4.self_attn.v_proj.weight
Trainable before PEFT: model.layers.4.self_attn.o_proj.weight
Trainable before PEFT: model.layers.4.mlp.gate_proj.weight
Trainable before PEFT: model.layers.4.mlp.up_proj.weight
Trainable before PEFT: model.layers.4.mlp.down_proj.weight
Trainable before PEFT: model.layers.4.input_layernorm.weight
Trainable before PEFT: model.layers.4.post_attention_layernorm.weight
Trainable before PEFT: model.layers.5.self_attn.q_proj.weight
Trainable before PEFT: model.layers.5.self_attn.k_proj.weight
Trainable before PEFT: model.layers.5.self_attn.v_proj.weight
Trainable before PEFT: model.layers.5.self_attn.o_proj.weight
Trainable before PEFT: model.layers.5.mlp.gate_proj.weight
Trainable before PEFT: model.layers.5.mlp.up_proj.weight
Trainable before PEFT: model.layers.5.mlp.down_proj.weight
Trainable before PEFT: model.layers.5.input_layernorm.weight
Trainable before PEFT: model.layers.5.post_attention_layernorm.weight
Trainable before PEFT: model.layers.6.self_attn.q_proj.weight
Trainable before PEFT: model.layers.6.self_attn.k_proj.weight
Trainable before PEFT: model.layers.6.self_attn.v_proj.weight
Trainable before PEFT: model.layers.6.self_attn.o_proj.weight
Trainable before PEFT: model.layers.6.mlp.gate_proj.weight
Trainable before PEFT: model.layers.6.mlp.up_proj.weight
Trainable before PEFT: model.layers.6.mlp.down_proj.weight
Trainable before PEFT: model.layers.6.input_layernorm.weight
Trainable before PEFT: model.layers.6.post_attention_layernorm.weight
Trainable before PEFT: model.layers.7.self_attn.q_proj.weight
Trainable before PEFT: model.layers.7.self_attn.k_proj.weight
Trainable before PEFT: model.layers.7.self_attn.v_proj.weight
Trainable before PEFT: model.layers.7.self_attn.o_proj.weight
Trainable before PEFT: model.layers.7.mlp.gate_proj.weight
Trainable before PEFT: model.layers.7.mlp.up_proj.weight
Trainable before PEFT: model.layers.7.mlp.down_proj.weight
Trainable before PEFT: model.layers.7.input_layernorm.weight
Trainable before PEFT: model.layers.7.post_attention_layernorm.weight
Trainable before PEFT: model.layers.8.self_attn.q_proj.weight
Trainable before PEFT: model.layers.8.self_attn.k_proj.weight
Trainable before PEFT: model.layers.8.self_attn.v_proj.weight
Trainable before PEFT: model.layers.8.self_attn.o_proj.weight
Trainable before PEFT: model.layers.8.mlp.gate_proj.weight
Trainable before PEFT: model.layers.8.mlp.up_proj.weight
Trainable before PEFT: model.layers.8.mlp.down_proj.weight
Trainable before PEFT: model.layers.8.input_layernorm.weight
Trainable before PEFT: model.layers.8.post_attention_layernorm.weight
Trainable before PEFT: model.layers.9.self_attn.q_proj.weight
Trainable before PEFT: model.layers.9.self_attn.k_proj.weight
Trainable before PEFT: model.layers.9.self_attn.v_proj.weight
Trainable before PEFT: model.layers.9.self_attn.o_proj.weight
Trainable before PEFT: model.layers.9.mlp.gate_proj.weight
Trainable before PEFT: model.layers.9.mlp.up_proj.weight
Trainable before PEFT: model.layers.9.mlp.down_proj.weight
Trainable before PEFT: model.layers.9.input_layernorm.weight
Trainable before PEFT: model.layers.9.post_attention_layernorm.weight
Trainable before PEFT: model.layers.10.self_attn.q_proj.weight
Trainable before PEFT: model.layers.10.self_attn.k_proj.weight
Trainable before PEFT: model.layers.10.self_attn.v_proj.weight
Trainable before PEFT: model.layers.10.self_attn.o_proj.weight
Trainable before PEFT: model.layers.10.mlp.gate_proj.weight
Trainable before PEFT: model.layers.10.mlp.up_proj.weight
Trainable before PEFT: model.layers.10.mlp.down_proj.weight
Trainable before PEFT: model.layers.10.input_layernorm.weight
Trainable before PEFT: model.layers.10.post_attention_layernorm.weight
Trainable before PEFT: model.layers.11.self_attn.q_proj.weight
Trainable before PEFT: model.layers.11.self_attn.k_proj.weight
Trainable before PEFT: model.layers.11.self_attn.v_proj.weight
Trainable before PEFT: model.layers.11.self_attn.o_proj.weight
Trainable before PEFT: model.layers.11.mlp.gate_proj.weight
Trainable before PEFT: model.layers.11.mlp.up_proj.weight
Trainable before PEFT: model.layers.11.mlp.down_proj.weight
Trainable before PEFT: model.layers.11.input_layernorm.weight
Trainable before PEFT: model.layers.11.post_attention_layernorm.weight
Trainable before PEFT: model.layers.12.self_attn.q_proj.weight
Trainable before PEFT: model.layers.12.self_attn.k_proj.weight
Trainable before PEFT: model.layers.12.self_attn.v_proj.weight
Trainable before PEFT: model.layers.12.self_attn.o_proj.weight
Trainable before PEFT: model.layers.12.mlp.gate_proj.weight
Trainable before PEFT: model.layers.12.mlp.up_proj.weight
Trainable before PEFT: model.layers.12.mlp.down_proj.weight
Trainable before PEFT: model.layers.12.input_layernorm.weight
Trainable before PEFT: model.layers.12.post_attention_layernorm.weight
Trainable before PEFT: model.layers.13.self_attn.q_proj.weight
Trainable before PEFT: model.layers.13.self_attn.k_proj.weight
Trainable before PEFT: model.layers.13.self_attn.v_proj.weight
Trainable before PEFT: model.layers.13.self_attn.o_proj.weight
Trainable before PEFT: model.layers.13.mlp.gate_proj.weight
Trainable before PEFT: model.layers.13.mlp.up_proj.weight
Trainable before PEFT: model.layers.13.mlp.down_proj.weight
Trainable before PEFT: model.layers.13.input_layernorm.weight
Trainable before PEFT: model.layers.13.post_attention_layernorm.weight
Trainable before PEFT: model.layers.14.self_attn.q_proj.weight
Trainable before PEFT: model.layers.14.self_attn.k_proj.weight
Trainable before PEFT: model.layers.14.self_attn.v_proj.weight
Trainable before PEFT: model.layers.14.self_attn.o_proj.weight
Trainable before PEFT: model.layers.14.mlp.gate_proj.weight
Trainable before PEFT: model.layers.14.mlp.up_proj.weight
Trainable before PEFT: model.layers.14.mlp.down_proj.weight
Trainable before PEFT: model.layers.14.input_layernorm.weight
Trainable before PEFT: model.layers.14.post_attention_layernorm.weight
Trainable before PEFT: model.layers.15.self_attn.q_proj.weight
Trainable before PEFT: model.layers.15.self_attn.k_proj.weight
Trainable before PEFT: model.layers.15.self_attn.v_proj.weight
Trainable before PEFT: model.layers.15.self_attn.o_proj.weight
Trainable before PEFT: model.layers.15.mlp.gate_proj.weight
Trainable before PEFT: model.layers.15.mlp.up_proj.weight
Trainable before PEFT: model.layers.15.mlp.down_proj.weight
Trainable before PEFT: model.layers.15.input_layernorm.weight
Trainable before PEFT: model.layers.15.post_attention_layernorm.weight
Trainable before PEFT: model.layers.16.self_attn.q_proj.weight
Trainable before PEFT: model.layers.16.self_attn.k_proj.weight
Trainable before PEFT: model.layers.16.self_attn.v_proj.weight
Trainable before PEFT: model.layers.16.self_attn.o_proj.weight
Trainable before PEFT: model.layers.16.mlp.gate_proj.weight
Trainable before PEFT: model.layers.16.mlp.up_proj.weight
Trainable before PEFT: model.layers.16.mlp.down_proj.weight
Trainable before PEFT: model.layers.16.input_layernorm.weight
Trainable before PEFT: model.layers.16.post_attention_layernorm.weight
Trainable before PEFT: model.layers.17.self_attn.q_proj.weight
Trainable before PEFT: model.layers.17.self_attn.k_proj.weight
Trainable before PEFT: model.layers.17.self_attn.v_proj.weight
Trainable before PEFT: model.layers.17.self_attn.o_proj.weight
Trainable before PEFT: model.layers.17.mlp.gate_proj.weight
Trainable before PEFT: model.layers.17.mlp.up_proj.weight
Trainable before PEFT: model.layers.17.mlp.down_proj.weight
Trainable before PEFT: model.layers.17.input_layernorm.weight
Trainable before PEFT: model.layers.17.post_attention_layernorm.weight
Trainable before PEFT: model.layers.18.self_attn.q_proj.weight
Trainable before PEFT: model.layers.18.self_attn.k_proj.weight
Trainable before PEFT: model.layers.18.self_attn.v_proj.weight
Trainable before PEFT: model.layers.18.self_attn.o_proj.weight
Trainable before PEFT: model.layers.18.mlp.gate_proj.weight
Trainable before PEFT: model.layers.18.mlp.up_proj.weight
Trainable before PEFT: model.layers.18.mlp.down_proj.weight
Trainable before PEFT: model.layers.18.input_layernorm.weight
Trainable before PEFT: model.layers.18.post_attention_layernorm.weight
Trainable before PEFT: model.layers.19.self_attn.q_proj.weight
Trainable before PEFT: model.layers.19.self_attn.k_proj.weight
Trainable before PEFT: model.layers.19.self_attn.v_proj.weight
Trainable before PEFT: model.layers.19.self_attn.o_proj.weight
Trainable before PEFT: model.layers.19.mlp.gate_proj.weight
Trainable before PEFT: model.layers.19.mlp.up_proj.weight
Trainable before PEFT: model.layers.19.mlp.down_proj.weight
Trainable before PEFT: model.layers.19.input_layernorm.weight
Trainable before PEFT: model.layers.19.post_attention_layernorm.weight
Trainable before PEFT: model.layers.20.self_attn.q_proj.weight
Trainable before PEFT: model.layers.20.self_attn.k_proj.weight
Trainable before PEFT: model.layers.20.self_attn.v_proj.weight
Trainable before PEFT: model.layers.20.self_attn.o_proj.weight
Trainable before PEFT: model.layers.20.mlp.gate_proj.weight
Trainable before PEFT: model.layers.20.mlp.up_proj.weight
Trainable before PEFT: model.layers.20.mlp.down_proj.weight
Trainable before PEFT: model.layers.20.input_layernorm.weight
Trainable before PEFT: model.layers.20.post_attention_layernorm.weight
Trainable before PEFT: model.layers.21.self_attn.q_proj.weight
Trainable before PEFT: model.layers.21.self_attn.k_proj.weight
Trainable before PEFT: model.layers.21.self_attn.v_proj.weight
Trainable before PEFT: model.layers.21.self_attn.o_proj.weight
Trainable before PEFT: model.layers.21.mlp.gate_proj.weight
Trainable before PEFT: model.layers.21.mlp.up_proj.weight
Trainable before PEFT: model.layers.21.mlp.down_proj.weight
Trainable before PEFT: model.layers.21.input_layernorm.weight
Trainable before PEFT: model.layers.21.post_attention_layernorm.weight
Trainable before PEFT: model.layers.22.self_attn.q_proj.weight
Trainable before PEFT: model.layers.22.self_attn.k_proj.weight
Trainable before PEFT: model.layers.22.self_attn.v_proj.weight
Trainable before PEFT: model.layers.22.self_attn.o_proj.weight
Trainable before PEFT: model.layers.22.mlp.gate_proj.weight
Trainable before PEFT: model.layers.22.mlp.up_proj.weight
Trainable before PEFT: model.layers.22.mlp.down_proj.weight
Trainable before PEFT: model.layers.22.input_layernorm.weight
Trainable before PEFT: model.layers.22.post_attention_layernorm.weight
Trainable before PEFT: model.layers.23.self_attn.q_proj.weight
Trainable before PEFT: model.layers.23.self_attn.k_proj.weight
Trainable before PEFT: model.layers.23.self_attn.v_proj.weight
Trainable before PEFT: model.layers.23.self_attn.o_proj.weight
Trainable before PEFT: model.layers.23.mlp.gate_proj.weight
Trainable before PEFT: model.layers.23.mlp.up_proj.weight
Trainable before PEFT: model.layers.23.mlp.down_proj.weight
Trainable before PEFT: model.layers.23.input_layernorm.weight
Trainable before PEFT: model.layers.23.post_attention_layernorm.weight
Trainable before PEFT: model.layers.24.self_attn.q_proj.weight
Trainable before PEFT: model.layers.24.self_attn.k_proj.weight
Trainable before PEFT: model.layers.24.self_attn.v_proj.weight
Trainable before PEFT: model.layers.24.self_attn.o_proj.weight
Trainable before PEFT: model.layers.24.mlp.gate_proj.weight
Trainable before PEFT: model.layers.24.mlp.up_proj.weight
Trainable before PEFT: model.layers.24.mlp.down_proj.weight
Trainable before PEFT: model.layers.24.input_layernorm.weight
Trainable before PEFT: model.layers.24.post_attention_layernorm.weight
Trainable before PEFT: model.layers.25.self_attn.q_proj.weight
Trainable before PEFT: model.layers.25.self_attn.k_proj.weight
Trainable before PEFT: model.layers.25.self_attn.v_proj.weight
Trainable before PEFT: model.layers.25.self_attn.o_proj.weight
Trainable before PEFT: model.layers.25.mlp.gate_proj.weight
Trainable before PEFT: model.layers.25.mlp.up_proj.weight
Trainable before PEFT: model.layers.25.mlp.down_proj.weight
Trainable before PEFT: model.layers.25.input_layernorm.weight
Trainable before PEFT: model.layers.25.post_attention_layernorm.weight
Trainable before PEFT: model.layers.26.self_attn.q_proj.weight
Trainable before PEFT: model.layers.26.self_attn.k_proj.weight
Trainable before PEFT: model.layers.26.self_attn.v_proj.weight
Trainable before PEFT: model.layers.26.self_attn.o_proj.weight
Trainable before PEFT: model.layers.26.mlp.gate_proj.weight
Trainable before PEFT: model.layers.26.mlp.up_proj.weight
Trainable before PEFT: model.layers.26.mlp.down_proj.weight
Trainable before PEFT: model.layers.26.input_layernorm.weight
Trainable before PEFT: model.layers.26.post_attention_layernorm.weight
Trainable before PEFT: model.layers.27.self_attn.q_proj.weight
Trainable before PEFT: model.layers.27.self_attn.k_proj.weight
Trainable before PEFT: model.layers.27.self_attn.v_proj.weight
Trainable before PEFT: model.layers.27.self_attn.o_proj.weight
Trainable before PEFT: model.layers.27.mlp.gate_proj.weight
Trainable before PEFT: model.layers.27.mlp.up_proj.weight
Trainable before PEFT: model.layers.27.mlp.down_proj.weight
Trainable before PEFT: model.layers.27.input_layernorm.weight
Trainable before PEFT: model.layers.27.post_attention_layernorm.weight
Trainable before PEFT: model.layers.28.self_attn.q_proj.weight
Trainable before PEFT: model.layers.28.self_attn.k_proj.weight
Trainable before PEFT: model.layers.28.self_attn.v_proj.weight
Trainable before PEFT: model.layers.28.self_attn.o_proj.weight
Trainable before PEFT: model.layers.28.mlp.gate_proj.weight
Trainable before PEFT: model.layers.28.mlp.up_proj.weight
Trainable before PEFT: model.layers.28.mlp.down_proj.weight
Trainable before PEFT: model.layers.28.input_layernorm.weight
Trainable before PEFT: model.layers.28.post_attention_layernorm.weight
Trainable before PEFT: model.layers.29.self_attn.q_proj.weight
Trainable before PEFT: model.layers.29.self_attn.k_proj.weight
Trainable before PEFT: model.layers.29.self_attn.v_proj.weight
Trainable before PEFT: model.layers.29.self_attn.o_proj.weight
Trainable before PEFT: model.layers.29.mlp.gate_proj.weight
Trainable before PEFT: model.layers.29.mlp.up_proj.weight
Trainable before PEFT: model.layers.29.mlp.down_proj.weight
Trainable before PEFT: model.layers.29.input_layernorm.weight
Trainable before PEFT: model.layers.29.post_attention_layernorm.weight
Trainable before PEFT: model.layers.30.self_attn.q_proj.weight
Trainable before PEFT: model.layers.30.self_attn.k_proj.weight
Trainable before PEFT: model.layers.30.self_attn.v_proj.weight
Trainable before PEFT: model.layers.30.self_attn.o_proj.weight
Trainable before PEFT: model.layers.30.mlp.gate_proj.weight
Trainable before PEFT: model.layers.30.mlp.up_proj.weight
Trainable before PEFT: model.layers.30.mlp.down_proj.weight
Trainable before PEFT: model.layers.30.input_layernorm.weight
Trainable before PEFT: model.layers.30.post_attention_layernorm.weight
Trainable before PEFT: model.layers.31.self_attn.q_proj.weight
Trainable before PEFT: model.layers.31.self_attn.k_proj.weight
Trainable before PEFT: model.layers.31.self_attn.v_proj.weight
Trainable before PEFT: model.layers.31.self_attn.o_proj.weight
Trainable before PEFT: model.layers.31.mlp.gate_proj.weight
Trainable before PEFT: model.layers.31.mlp.up_proj.weight
Trainable before PEFT: model.layers.31.mlp.down_proj.weight
Trainable before PEFT: model.layers.31.input_layernorm.weight
Trainable before PEFT: model.layers.31.post_attention_layernorm.weight
Trainable before PEFT: model.norm.weight
Trainable before PEFT: lm_head.weight
Trainable before PEFT: mm_gated_cross_attention.ff_gate
Trainable before PEFT: mm_gated_cross_attention.norm_query.weight
Trainable before PEFT: mm_gated_cross_attention.norm_query.bias
Trainable before PEFT: mm_gated_cross_attention.norm_key_value.weight
Trainable before PEFT: mm_gated_cross_attention.norm_key_value.bias
Trainable before PEFT: mm_gated_cross_attention.to_q_cross.weight
Trainable before PEFT: mm_gated_cross_attention.to_kv_cross.weight
Trainable before PEFT: mm_gated_cross_attention.to_qkv_self_attn.weight
Trainable before PEFT: mm_gated_cross_attention.gate_norm.weight
Trainable before PEFT: mm_gated_cross_attention.gate_norm.bias
Trainable before PEFT: mm_gated_cross_attention.gate_attention.in_proj_weight
Trainable before PEFT: mm_gated_cross_attention.gate_attention.out_proj.weight
Trainable before PEFT: mm_gated_cross_attention.to_mixing_weights.0.weight
Trainable before PEFT: mm_gated_cross_attention.to_mixing_weights.0.bias
Trainable before PEFT: mm_gated_cross_attention.to_out.weight
Trainable before PEFT: mm_gated_cross_attention.ff.norm.weight
Trainable before PEFT: mm_gated_cross_attention.ff.norm.bias
Trainable before PEFT: mm_gated_cross_attention.ff.fc1.weight
Trainable before PEFT: mm_gated_cross_attention.ff.fc2.weight
Trainable before PEFT: mm_resampler.latents
Trainable before PEFT: mm_resampler.latent_pos_emb
Trainable before PEFT: mm_resampler.layers.0.0.norm_media.weight
Trainable before PEFT: mm_resampler.layers.0.0.norm_media.bias
Trainable before PEFT: mm_resampler.layers.0.0.norm_latents.weight
Trainable before PEFT: mm_resampler.layers.0.0.norm_latents.bias
Trainable before PEFT: mm_resampler.layers.0.0.to_q.weight
Trainable before PEFT: mm_resampler.layers.0.0.to_kv.weight
Trainable before PEFT: mm_resampler.layers.0.0.to_out.weight
Trainable before PEFT: mm_resampler.layers.0.1.norm.weight
Trainable before PEFT: mm_resampler.layers.0.1.norm.bias
Trainable before PEFT: mm_resampler.layers.0.1.fc1.weight
Trainable before PEFT: mm_resampler.layers.0.1.fc2.weight
Trainable before PEFT: mm_resampler.layers.1.0.norm_media.weight
Trainable before PEFT: mm_resampler.layers.1.0.norm_media.bias
Trainable before PEFT: mm_resampler.layers.1.0.norm_latents.weight
Trainable before PEFT: mm_resampler.layers.1.0.norm_latents.bias
Trainable before PEFT: mm_resampler.layers.1.0.to_q.weight
Trainable before PEFT: mm_resampler.layers.1.0.to_kv.weight
Trainable before PEFT: mm_resampler.layers.1.0.to_out.weight
Trainable before PEFT: mm_resampler.layers.1.1.norm.weight
Trainable before PEFT: mm_resampler.layers.1.1.norm.bias
Trainable before PEFT: mm_resampler.layers.1.1.fc1.weight
Trainable before PEFT: mm_resampler.layers.1.1.fc2.weight
Trainable before PEFT: mm_resampler.norm.weight
Trainable before PEFT: mm_resampler.norm.bias
Trainable before PEFT: mm_resampler.proj_out.weight
Trainable before PEFT: mm_resampler.proj_out.bias
Trainable before PEFT: mm_projector.linear1.weight
Trainable before PEFT: mm_projector.linear1.bias
Trainable before PEFT: mm_projector.linear2.weight
Trainable before PEFT: mm_projector.linear2.bias
LoRA adapted model created with the following PEFT config:
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Meta-Llama-3.1-8B-Instruct', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=16, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)
trainable params: 13,631,488 || all params: 9,478,208,583 || trainable%: 0.14381924475105212

[RESTORING ADAPTER TRAINABLE STATE]
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.ff_gate
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.norm_query.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.norm_query.bias
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.norm_key_value.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.norm_key_value.bias
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.to_q_cross.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.to_kv_cross.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.to_qkv_self_attn.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.gate_norm.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.gate_norm.bias
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.gate_attention.in_proj_weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.gate_attention.out_proj.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.to_mixing_weights.0.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.to_mixing_weights.0.bias
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.to_out.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.ff.norm.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.ff.norm.bias
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.ff.fc1.weight
Restoring trainable state for adapter param: base_model.model.mm_gated_cross_attention.ff.fc2.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.latents
Restoring trainable state for adapter param: base_model.model.mm_resampler.latent_pos_emb
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.0.norm_media.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.0.norm_media.bias
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.0.norm_latents.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.0.norm_latents.bias
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.0.to_q.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.0.to_kv.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.0.to_out.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.1.norm.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.1.norm.bias
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.1.fc1.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.0.1.fc2.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.0.norm_media.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.0.norm_media.bias
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.0.norm_latents.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.0.norm_latents.bias
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.0.to_q.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.0.to_kv.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.0.to_out.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.1.norm.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.1.norm.bias
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.1.fc1.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.layers.1.1.fc2.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.norm.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.norm.bias
Restoring trainable state for adapter param: base_model.model.mm_resampler.proj_out.weight
Restoring trainable state for adapter param: base_model.model.mm_resampler.proj_out.bias
Restoring trainable state for adapter param: base_model.model.mm_projector.linear1.weight
Restoring trainable state for adapter param: base_model.model.mm_projector.linear1.bias
Restoring trainable state for adapter param: base_model.model.mm_projector.linear2.weight
Restoring trainable state for adapter param: base_model.model.mm_projector.linear2.bias

Final trainable parameters summary:
Number of trainable parameters: 307
Sample trainable parameters: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight']
LoRA applied.
Lora target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 46,178,819 || all params: 9,478,208,583 || trainable%: 0.48721041107731866

[FINAL ADAPTER VERIFICATION]
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.ff_gate
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.norm_query.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.norm_query.bias
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.norm_key_value.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.norm_key_value.bias
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.to_q_cross.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.to_kv_cross.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.to_qkv_self_attn.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.gate_norm.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.gate_norm.bias
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.gate_attention.in_proj_weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.gate_attention.out_proj.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.to_mixing_weights.0.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.to_mixing_weights.0.bias
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.to_out.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.ff.norm.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.ff.norm.bias
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.ff.fc1.weight
Adapter parameter trainable: base_model.model.mm_gated_cross_attention.ff.fc2.weight
Adapter parameter trainable: base_model.model.mm_resampler.latents
Adapter parameter trainable: base_model.model.mm_resampler.latent_pos_emb
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.0.norm_media.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.0.norm_media.bias
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.0.norm_latents.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.0.norm_latents.bias
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.0.to_q.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.0.to_kv.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.0.to_out.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.1.norm.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.1.norm.bias
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.1.fc1.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.0.1.fc2.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.0.norm_media.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.0.norm_media.bias
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.0.norm_latents.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.0.norm_latents.bias
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.0.to_q.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.0.to_kv.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.0.to_out.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.1.norm.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.1.norm.bias
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.1.fc1.weight
Adapter parameter trainable: base_model.model.mm_resampler.layers.1.1.fc2.weight
Adapter parameter trainable: base_model.model.mm_resampler.norm.weight
Adapter parameter trainable: base_model.model.mm_resampler.norm.bias
Adapter parameter trainable: base_model.model.mm_resampler.proj_out.weight
Adapter parameter trainable: base_model.model.mm_resampler.proj_out.bias
Adapter parameter trainable: base_model.model.mm_projector.linear1.weight
Adapter parameter trainable: base_model.model.mm_projector.linear1.bias
Adapter parameter trainable: base_model.model.mm_projector.linear2.weight
Adapter parameter trainable: base_model.model.mm_projector.linear2.bias
Number of trainable parameters after LoRA: 46178819
[DEBUG] Initializing training dataset.
[DEBUG] Using data path: ./data/test.json
[DEBUG] Tokenizer model: meta-llama/Meta-Llama-3.1-8B-Instruct
[DEBUG] Max text length: 1536
[DEBUG] processed_record: {'wild_type_seq': 'MQPWHGKAMQRASEAGATAPKASARNARGAPMDPTESPAAPEAALPKAGKFGPARKSGSRQKKSAPDTQERPPVRATGARAKKAPQRAQDTQPSDATSAPGAEGLEPPAAREPALSRAGSCRQRGARCSTKPRPPPGPWDVPSPGLPVSAPILVRRDAAPGASKLRAVLEKLKLSRDDISTAAGMVKGVVDHLLLRLKCDSAFRGVGLLNTGSYYEHVKISAPNEFDVMFKLEVPRIQLEEYSNTRAYYFVKFKRNPKENPLSQFLEGEILSASKMLSKFRKIIKEEINDIKDTDVIMKRKRGGSPAVTLLISEKISVDITLALESKSSWPASTQEGLRIQNWLSAKVRKQLRLKPFYLVPKHAKEGNGFQEETWRLSFSHIEKEILNNHGKSKTCCENKEEKCCRKDCLKLMKYLLEQLKERFKDKKHLDKFSSYHVKTAFFHVCTQNPQDSQWDRKDLGLCFDNCVTYFLQCLRTEKLENYFIPEFNLFSSNLIDKRSKEFLTKQIEYERNNEFPVFDEF', 'mutation_seq': 'MQPWHGKAMQRASEAGATAPKASARNARGAPMDPTESPAAPEAALPKAGKFGPARKSGSRQKKSAPDTQERPPVRATGARAKKAPQRAQDTQPSAATSAPGAEGLEPPAAREPALSRAGSCRQRGARCSTKPRPPPGPWDVPSPGLPVSAPILVRRDAAPGASKLRAVLEKLKLSRDDISTAAGMVKGVVDHLLLRLKCDSAFRGVGLLNTGSYYEHVKISAPNEFDVMFKLEVPRIQLEEYSNTRAYYFVKFKRNPKENPLSQFLEGEILSASKMLSKFRKIIKEEINDIKDTDVIMKRKRGGSPAVTLLISEKISVDITLALESKSSWPASTQEGLRIQNWLSAKVRKQLRLKPFYLVPKHAKEGNGFQEETWRLSFSHIEKEILNNHGKSKTCCENKEEKCCRKDCLKLMKYLLEQLKERFKDKKHLDKFSSYHVKTAFFHVCTQNPQDSQWDRKDLGLCFDNCVTYFLQCLRTEKLENYFIPEFNLFSSNLIDKRSKEFLTKQIEYERNNEFPVFDEF', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Aspartic acid to Alanine at position 95, the original function of the non-mutated protein is as follows: Nucleotidyltransferase that catalyzes the formation of cyclic GMP-AMP (2',3'-cGAMP) from ATP and GTP and plays a key role in innate immunity. Catalysis involves both the formation of a 2',5' phosphodiester linkage at the GpA step and the formation of a 3',5' phosphodiester linkage at the ApG step, producing c[G(2',5')pA(3',5')p]. Acts as a key DNA sensor: directly binds double-stranded DNA (dsDNA), inducing the formation of liquid-like droplets in which CGAS is activated, leading to synthesis of 2',3'-cGAMP, a second messenger that binds to and activates STING1, thereby triggering type-I interferon production. Preferentially recognizes and binds curved long dsDNAs of a minimal length of 40 bp. Acts as a key foreign DNA sensor, the presence of double-stranded DNA (dsDNA) in the cytoplasm being a danger signal that triggers the immune responses. Has antiviral activity by sensing the presence of dsDNA from DNA viruses in the cytoplasm. Also acts as an innate immune sensor of infection by retroviruses, such as HIV-2, by detecting the presence of reverse-transcribed DNA in the cytosol. In contrast, HIV-1 is poorly sensed by CGAS, due to its capsid that cloaks viral DNA from CGAS detection. Detection of retroviral reverse-transcribed DNA in the cytosol may be indirect and be mediated via interaction with PQBP1, which directly binds reverse-transcribed retroviral DNA. Also detects the presence of DNA from bacteria, such as M.tuberculosis. 2',3'-cGAMP can be transferred from producing cells to neighboring cells through gap junctions, leading to promote STING1 activation and convey immune response to connecting cells. 2',3'-cGAMP can also be transferred between cells by virtue of packaging within viral particles contributing to IFN-induction in newly infected cells in a cGAS-independent but STING1-dependent manner. Also senses the presence of neutrophil extracellular traps (NETs) that are translocated to the cytosol following phagocytosis, leading to synthesis of 2',3'-cGAMP. In addition to foreign DNA, can also be activated by endogenous nuclear or mitochondrial DNA. When self-DNA leaks into the cytosol during cellular stress (such as mitochondrial stress, SARS-CoV-2 infection causing severe COVID-19 disease, DNA damage, mitotic arrest or senescence), or is present in form of cytosolic micronuclei, CGAS is activated leading to a state of sterile inflammation. Acts as a regulator of cellular senescence by binding to cytosolic chromatin fragments that are present in senescent cells, leading to trigger type-I interferon production via STING1 and promote cellular senescence. Also involved in the inflammatory response to genome instability and double-stranded DNA breaks: acts by localizing to micronuclei arising from genome instability. Micronuclei, which are frequently found in cancer cells, consist of chromatin surrounded by their own nuclear membrane: following breakdown of the micronuclear envelope, a process associated with chromothripsis, CGAS binds self-DNA exposed to the cytosol, leading to 2',3'-cGAMP synthesis and subsequent activation of STING1 and type-I interferon production. Activated in response to prolonged mitotic arrest, promoting mitotic cell death. In a healthy cell, CGAS is however kept inactive even in cellular events that directly expose it to self-DNA, such as mitosis, when cGAS associates with chromatin directly after nuclear envelope breakdown or remains in the form of postmitotic persistent nuclear cGAS pools bound to chromatin. Nuclear CGAS is inactivated by chromatin via direct interaction with nucleosomes, which block CGAS from DNA binding and thus prevent CGAS-induced autoimmunity. Also acts as a suppressor of DNA repair in response to DNA damage: inhibits homologous recombination repair by interacting with PARP1, the CGAS-PARP1 interaction leading to impede the formation of the PARP1-TIMELESS complex. In addition to DNA, also sense translation stress: in response to translation stress, translocates to the cytosol and associates with collided ribosomes, promoting its activation and triggering type-I interferon production. In contrast to other mammals, human CGAS displays species-specific mechanisms of DNA recognition and produces less 2',3'-cGAMP, allowing a more fine-tuned response to pathogens.", 'gpt_response': 'No effect on type I IFN and RSAD2 induction. No effect on cleavage by CASP1; when associated with A-33; A-67 and A-90. Highly decreases cleavage by CASP1 and enhances RSAD2 induction upon DNA virus infection; when associated with A-33; A-67; A-90 and A-140. Abolishes cleavage by CASP1 and enhances RSAD2 induction upon DNA virus infection; when associated with A-33; A-67; A-90; A-140 and A-157.', 'id': 'Q8N884-D95A'}
[DEBUG] processed_record: {'wild_type_seq': 'MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSLHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL', 'mutation_seq': 'MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSFHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'In BCHED; seems to cause reduced expression of the protein. The mutation in the BCHE gene leads to reduced expression of BChE protein.', 'id': 'P06276-L153F'}
[DEBUG] processed_record: {'wild_type_seq': 'MGCTLSAEDKAAVERSKMIDRNLREDGEKAAREVKLLLLGAGESGKSTIVKQMKIIHEAGYSEEECKQYKAVVYSNTIQSIIAIIRAMGRLKIDFGDSARADDARQLFVLAGAAEEGFMTAELAGVIKRLWKDSGVQACFNRSREYQLNDSAAYYLNDLDRIAQPNYIPTQQDVLRTRVKTTGIVETHFTFKDLHFKMFDVGGQRSERKKWIHCFEGVTAIIFCVALSDYDLVLAEDEEMNRMHESMKLFDSICNNKWFTDTSIILFLNKKDLFEEKIKKSPLTICYPEYAGSNTYEEAAAYIQCQFEDLNKRKDTKEIYTHFTCATDTKNVQFVFDAVTDVIIKNNLKDCGLF', 'mutation_seq': None, 'human_query': '<wt_protein> </wt_protein> Describe this protein?', 'gpt_response': 'Enhances interaction (inactive GDP-bound) with RGS14.', 'id': 'P63096-E245L'}
[DEBUG] processed_record: {'wild_type_seq': 'MPGGLLLGDEAPNFEANTTIGHIRFHDFLGDSWGILFSHPRDFTPVCTTELGRAAKLAPEFAKRNVKLIALSIDSVEDHFAWSKDINAYNGAAPTEKLPFPIIDDKDRDLAILLGMLDPAEKDEKGMPVTARVVFIFGPDKKLKLSILYPATTGRNFDEILRVVDSLQLTASNPVATPVDWKKGESVMVLPTLPEEEAKQLFPKGVFTKELPSGKKYLRYTPQP', 'mutation_seq': 'MPGGLLLGDEAPNFEANTTIGHIRFHDFLGDAWGILFSHPRDFTPVCTTELGRAAKLAPEFAKRNVKLIALSIDSVEDHFAWSKDINAYNGAAPTEKLPFPIIDDKDRDLAILLGMLDPAEKDEKGMPVTARVVFIFGPDKKLKLSILYPATTGRNFDEILRVVDSLQLTASNPVATPVDWKKGESVMVLPTLPEEEAKQLFPKGVFTKELPSGKKYLRYTPQP', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'Abolishes lipid binding. Increased Prdx6 alpha-helical content, key role in maintaining Prdx6 structure and for phospholipid substrate binding.', 'id': 'O35244-S32A'}
[DEBUG] processed_record: {'wild_type_seq': 'MERWPWPSGGAWLLVAARALLQLLRSDLRLGRPLLAALALLAALDWLCQRLLPPPAALAVLAAAGWIALSRLARPQRLPVATRAVLITGCDSGFGKETAKKLDSMGFTVLATVLELNSPGAIELRTCCSPRLRLLQMDLTKPGDISRVLEFTKAHTTSTGLWGLVNNAGHNEVVADAELSPVATFRSCMEVNFFGALELTKGLLPLLRSSRGRIVTVGSPAGDMPYPCLGAYGTSKAAVALLMDTFSCELLPWGVKVSIIQPGCFKTESVRNVGQWEKRKQLLLANLPQELLQAYGKDYIEHLHGQFLHSLRLAMSDLTPVVDAITDALLAARPRRRYYPGQGLGLMYFIHYYLPEGLRRRFLQAFFISHCLPRALQPGQPGTTPPQDAAQDPNLSPGPSPAVAR', 'mutation_seq': 'MERWPWPSGGAWLLVAARALLQLLRSDLRLGRPLLAALALLAALDWLCQRLLPPPAALAVLAAAGWIALSRLARPQRLPVATRAVLITGCDSGFGKETAKKLDSMGFTVLATVLELNSPGAIELRTCCSPRLRLLQMDLTKPGDISRVLEFTKAHTTSTGLWGLVNNAGHNEVVADAELSPVATFRSCMEVNFFGALELTKGLLPLLRSSRGRIVTVGSPAGDMPYPCLGAYGTSKAAVALLMDTFSCELLPWGVKVSIIQPGCFKTESVRNVGQWEKRKQLLLANLPQELLQAYGKDYIEHLHGQFLHSLRLAMSDLTPVVDAITDALLAARPRRCYYPGQGLGLMYFIHYYLPEGLRRRFLQAFFISHCLPRALQPGQPGTTPPQDAAQDPNLSPGPSPAVAR', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.", 'gpt_response': 'In AME; decreased half-life from 21 to 4 hours compared to wild-type, probably due to degradation via the proteasomal pathway. This mutation has been discovered in a consanguineous Iranian family with three siblings suffering from Apparent Mineralocorticoid Excess (AME). The mutation in the HSD11B2 gene is associated with apparent mineralocorticoid excess (AME). This mutation results in a deficiency of 11 beta-hydroxysteroid dehydrogenase type 2 enzyme activity, leading to pre- and postnatal growth failure, juvenile hypertension, hypokalemic metabolic alkalosis, and hyporeninemic hypoaldosteronism. The 11 beta HSD2 enzyme is responsible for the conversion of cortisol to the inactive metabolite cortisone, and the mutation impairs this conversion, causing mineralocorticoid receptors to be exposed to excessive cortisol levels.', 'id': 'P80365-R337C'}
[DEBUG] processed_record: {'wild_type_seq': 'MLTCEMRESALARLGRALADPTRCRILVALLDGVCYPGQLAAHLGLTRSNVSNHLSCLRGCGLVVATYEGRQVRYALADSHLARALGELVQVVLAVDTDQPCVAERAASGEAVEMTGS', 'mutation_seq': 'MLTSEMRESALARLGRALADPTRCRILVALLDGVCYPGQLAAHLGLTRSNVSNHLSCLRGCGLVVATYEGRQVRYALADSHLARALGELVQVVLAVDTDQPCVAERAASGEAVEMTGS', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.", 'gpt_response': 'No effect. Substitution of C4 with S is obligatory for Cd(II) detection.', 'id': 'P9WMI9-C4S'}
[DEBUG] processed_record: {'wild_type_seq': 'MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKTGLRVAVKKLSRPFQSIIHAKRTYRELRLLKHMKHENVIGLLDVFTPARSLEEFNDVYLVTHLMGADLNNIVKCQKLTDDHVQFLIYQILRGLKYIHSADIIHRDLKPSNLAVNEDCELKILDFGLARHTDDEMTGYVATRWYRAPEIMLNWMHYNQTVDIWSVGCIMAELLTGRTLFPGTDHIDQLKLILRLVGTPGAELLKKISSESARNYIQSLTQMPKMNFANVFIGANPLAVDLLEKMLVLDSDKRITAAQALAHAYFAQYHDPDDEPVADPYDQSFESRDLLIDEWKSLTYDEVISFVPPPLDQEEMES', 'mutation_seq': 'MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKTGLRVAVKKLSRPFQSIIHAKRTYRELRLLKHMKHENVIGLLDVFTPARSLEEFNDVYLVTHLMGADLNNIVKCQKLTDDHVQFLIYQILRGLKYIHSADIIHRDLKPSNLAVNEDCELKILDFGLARHADDEMTGYVATRWYRAPEIMLNWMHYNQTVDIWSVGCIMAELLTGRTLFPGTDHIDQLKLILRLVGTPGAELLKKISSESARNYIQSLTQMPKMNFANVFIGANPLAVDLLEKMLVLDSDKRITAAQALAHAYFAQYHDPDDEPVADPYDQSFESRDLLIDEWKSLTYDEVISFVPPPLDQEEMES', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'No effect on either the kinase activity or tyrosine phosphorylation.', 'id': 'Q16539-T175A'}
[DEBUG] processed_record: {'wild_type_seq': 'MAFMKKYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGIGREMAYHLAKMGAHVVVTARSKETLQKVVSHCLELGAASAHYIAGTMEDMTFAEQFVAQAGKLMGGLDMLILNHITNTSLNLFHDDIHHVRKSMEVNFLSYVVLTVAALPMLKQSNGSIVVVSSLAGKVAYPMVAAYSASKFALDGFFSSIRKEYSVSRVNVSITLCVLGLIDTETAMKAVSGIVHMQAAPKEECALEIIKGGALRQEEVYYDSSLWTTLLIRNPCRKILEFLYSTSYNMDRFINK', 'mutation_seq': 'MAFMKSYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGIGREMAYHLAKMGAHVVVTARSKETLQKVVSHCLELGAASAHYIAGTMEDMTFAEQFVAQAGKLMGGLDMLILNHITNTSLNLFHDDIHHVRKSMEVNFLSYVVLTVAALPMLKQSNGSIVVVSSLAGKVAYPMVAAYSASKFALDGFFSSIRKEYSVSRVNVSITLCVLGLIDTETAMKAVSGIVHMQAAPKEECALEIIKGGALRQEEVYYDSSLWTTLLIRNPCRKILEFLYSTSYNMDRFINK', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.", 'gpt_response': 'No effect on topology or activity. Mutation of K(6) to S did not affect the subcellular localization or catalytic activity of 11beta-HSD1.', 'id': 'P28845-K6S'}
[DEBUG] processed_record: {'wild_type_seq': 'MAPLHHILVLCVGFLTTATAEAPQEHDPFTYDYQSLRIGGLIIAGILFILGILIVLSRRCRCKFNQQQRTGEPDEEEGTFRSSIRRLSTRRR', 'mutation_seq': None, 'human_query': '<wt_protein> </wt_protein> Describe this protein?', 'gpt_response': 'Loss of glutathionylation and loss of ability to reduce glutathionylation of ATP1B1. The mutation in the FXYD protein family affects the reactivity of the Na(+)-K(+) pump and modulates its activity.', 'id': 'P56513-K63G'}
[DEBUG] processed_record: {'wild_type_seq': 'MGLFNAHAVAQQRADRIATLLQSFADGQLDTAVGEAPAPGYERLYDSLRALQRQLREQRAELQQVESLEAGLAEMSRQHEAGWIDQTIPAERLEGRAARIAKGVNELVAAHIAVKMKVVSVVTAYGQGNFEPLMDRLPGKKAQITEAIDGVRERLRGAAEATSAQLATAAYNARIKSALDNVSANVMIADNDLNIIYMNRTVSEMLGRAEADIRKQLPNFDAGRLMGANIDVFHKNPAHQRHLLANLTGVHKAELNLGGRRFSLDVVPVFNDANERLGSAVQWTDRTEEHRAEQEVSQLVQAAAAGDFSKRVEEAGKEGFFLRLAKDLNSLVDTADRGLRDVSRMLGALAQGDLTQRIEADYQGTFGQLKDFSNDTAQSLSRMLGQIREAADTINTAASEIASGNAELSARTEQQASSLEETASSMEELTSTVKLNAENARQANSLAANASEVATQGGTVVQKVVSTMSSINESARKIADIIGVIDGIAFQTNILALNAAVEAARAGEQGRGFAVVAGEVRTLAQRSAAAAKEIKTLISDSVDKVENGNTLVAQAGQTMSDIVVAIRRVTDIMSEIAAASAEQSTGIEEVNSAVSQMDDMTQQNAALVEEAAAAAEAMQEQAGLLNQSVAVFRLDTPPSVVQLASARPSAPRPSAPAPLARSGMARASKARKEDGWEEF', 'mutation_seq': 'MGLFNAHAVAQQRADRIATLLQSFADGQLDTAVGEAPAPGYERLYDSLRALQRQLREQRAELQQVESLEAGLAEMSRQHEAGWIDQTIPAERLEGRAARIAKGVNELVAAHIAVKMKVVSVVTAYGQGNFEPLMDRLPGKKAQITEAIDGVRERLRGAAEATSAQLATAAYNARIKSALDNVSANVMIADNDLNIIYMNRTVSEMLGRAEADIRKQLPNFDAGRLMGANIDVFHKNPAHQRHLLANLTGVHKAELNLGGRRFSKDVVPVFNDANERLGSAVQWTDRTEEHRAEQEVSQLVQAAAAGDFSKRVEEAGKEGFFLRLAKDLNSLVDTADRGLRDVSRMLGALAQGDLTQRIEADYQGTFGQLKDFSNDTAQSLSRMLGQIREAADTINTAASEIASGNAELSARTEQQASSLEETASSMEELTSTVKLNAENARQANSLAANASEVATQGGTVVQKVVSTMSSINESARKIADIIGVIDGIAFQTNILALNAAVEAARAGEQGRGFAVVAGEVRTLAQRSAAAAKEIKTLISDSVDKVENGNTLVAQAGQTMSDIVVAIRRVTDIMSEIAAASAEQSTGIEEVNSAVSQMDDMTQQNAALVEEAAAAAEAMQEQAGLLNQSVAVFRLDTPPSVVQLASARPSAPRPSAPAPLARSGMARASKARKEDGWEEF', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.", 'gpt_response': 'Signal-off mutant that does not respond to the addition or removal of O(2). Does not bind either O(2) or CO. formed a hexacoordinate heme.', 'id': 'Q9I6V6-L264K'}
[DEBUG] processed_record: {'wild_type_seq': 'MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSLHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL', 'mutation_seq': 'MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSFHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'In BCHED; seems to cause reduced expression of the protein. The mutation in the BCHE gene leads to reduced expression of BChE protein.', 'id': 'P06276-L153F'}
[DEBUG] processed_record: {'wild_type_seq': 'MGCTLSAEDKAAVERSKMIDRNLREDGEKAAREVKLLLLGAGESGKSTIVKQMKIIHEAGYSEEECKQYKAVVYSNTIQSIIAIIRAMGRLKIDFGDSARADDARQLFVLAGAAEEGFMTAELAGVIKRLWKDSGVQACFNRSREYQLNDSAAYYLNDLDRIAQPNYIPTQQDVLRTRVKTTGIVETHFTFKDLHFKMFDVGGQRSERKKWIHCFEGVTAIIFCVALSDYDLVLAEDEEMNRMHESMKLFDSICNNKWFTDTSIILFLNKKDLFEEKIKKSPLTICYPEYAGSNTYEEAAAYIQCQFEDLNKRKDTKEIYTHFTCATDTKNVQFVFDAVTDVIIKNNLKDCGLF', 'mutation_seq': None, 'human_query': '<wt_protein> </wt_protein> Describe this protein?', 'gpt_response': 'Enhances interaction (inactive GDP-bound) with RGS14.', 'id': 'P63096-E245L'}
[DEBUG] processed_record: {'wild_type_seq': 'MPGGLLLGDEAPNFEANTTIGHIRFHDFLGDSWGILFSHPRDFTPVCTTELGRAAKLAPEFAKRNVKLIALSIDSVEDHFAWSKDINAYNGAAPTEKLPFPIIDDKDRDLAILLGMLDPAEKDEKGMPVTARVVFIFGPDKKLKLSILYPATTGRNFDEILRVVDSLQLTASNPVATPVDWKKGESVMVLPTLPEEEAKQLFPKGVFTKELPSGKKYLRYTPQP', 'mutation_seq': 'MPGGLLLGDEAPNFEANTTIGHIRFHDFLGDAWGILFSHPRDFTPVCTTELGRAAKLAPEFAKRNVKLIALSIDSVEDHFAWSKDINAYNGAAPTEKLPFPIIDDKDRDLAILLGMLDPAEKDEKGMPVTARVVFIFGPDKKLKLSILYPATTGRNFDEILRVVDSLQLTASNPVATPVDWKKGESVMVLPTLPEEEAKQLFPKGVFTKELPSGKKYLRYTPQP', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'Abolishes lipid binding. Increased Prdx6 alpha-helical content, key role in maintaining Prdx6 structure and for phospholipid substrate binding.', 'id': 'O35244-S32A'}
[DEBUG] processed_record: {'wild_type_seq': 'MERWPWPSGGAWLLVAARALLQLLRSDLRLGRPLLAALALLAALDWLCQRLLPPPAALAVLAAAGWIALSRLARPQRLPVATRAVLITGCDSGFGKETAKKLDSMGFTVLATVLELNSPGAIELRTCCSPRLRLLQMDLTKPGDISRVLEFTKAHTTSTGLWGLVNNAGHNEVVADAELSPVATFRSCMEVNFFGALELTKGLLPLLRSSRGRIVTVGSPAGDMPYPCLGAYGTSKAAVALLMDTFSCELLPWGVKVSIIQPGCFKTESVRNVGQWEKRKQLLLANLPQELLQAYGKDYIEHLHGQFLHSLRLAMSDLTPVVDAITDALLAARPRRRYYPGQGLGLMYFIHYYLPEGLRRRFLQAFFISHCLPRALQPGQPGTTPPQDAAQDPNLSPGPSPAVAR', 'mutation_seq': 'MERWPWPSGGAWLLVAARALLQLLRSDLRLGRPLLAALALLAALDWLCQRLLPPPAALAVLAAAGWIALSRLARPQRLPVATRAVLITGCDSGFGKETAKKLDSMGFTVLATVLELNSPGAIELRTCCSPRLRLLQMDLTKPGDISRVLEFTKAHTTSTGLWGLVNNAGHNEVVADAELSPVATFRSCMEVNFFGALELTKGLLPLLRSSRGRIVTVGSPAGDMPYPCLGAYGTSKAAVALLMDTFSCELLPWGVKVSIIQPGCFKTESVRNVGQWEKRKQLLLANLPQELLQAYGKDYIEHLHGQFLHSLRLAMSDLTPVVDAITDALLAARPRRCYYPGQGLGLMYFIHYYLPEGLRRRFLQAFFISHCLPRALQPGQPGTTPPQDAAQDPNLSPGPSPAVAR', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.", 'gpt_response': 'In AME; decreased half-life from 21 to 4 hours compared to wild-type, probably due to degradation via the proteasomal pathway. This mutation has been discovered in a consanguineous Iranian family with three siblings suffering from Apparent Mineralocorticoid Excess (AME). The mutation in the HSD11B2 gene is associated with apparent mineralocorticoid excess (AME). This mutation results in a deficiency of 11 beta-hydroxysteroid dehydrogenase type 2 enzyme activity, leading to pre- and postnatal growth failure, juvenile hypertension, hypokalemic metabolic alkalosis, and hyporeninemic hypoaldosteronism. The 11 beta HSD2 enzyme is responsible for the conversion of cortisol to the inactive metabolite cortisone, and the mutation impairs this conversion, causing mineralocorticoid receptors to be exposed to excessive cortisol levels.', 'id': 'P80365-R337C'}
[DEBUG] processed_record: {'wild_type_seq': 'MLTCEMRESALARLGRALADPTRCRILVALLDGVCYPGQLAAHLGLTRSNVSNHLSCLRGCGLVVATYEGRQVRYALADSHLARALGELVQVVLAVDTDQPCVAERAASGEAVEMTGS', 'mutation_seq': 'MLTSEMRESALARLGRALADPTRCRILVALLDGVCYPGQLAAHLGLTRSNVSNHLSCLRGCGLVVATYEGRQVRYALADSHLARALGELVQVVLAVDTDQPCVAERAASGEAVEMTGS', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.", 'gpt_response': 'No effect. Substitution of C4 with S is obligatory for Cd(II) detection.', 'id': 'P9WMI9-C4S'}
[DEBUG] processed_record: {'wild_type_seq': 'MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKTGLRVAVKKLSRPFQSIIHAKRTYRELRLLKHMKHENVIGLLDVFTPARSLEEFNDVYLVTHLMGADLNNIVKCQKLTDDHVQFLIYQILRGLKYIHSADIIHRDLKPSNLAVNEDCELKILDFGLARHTDDEMTGYVATRWYRAPEIMLNWMHYNQTVDIWSVGCIMAELLTGRTLFPGTDHIDQLKLILRLVGTPGAELLKKISSESARNYIQSLTQMPKMNFANVFIGANPLAVDLLEKMLVLDSDKRITAAQALAHAYFAQYHDPDDEPVADPYDQSFESRDLLIDEWKSLTYDEVISFVPPPLDQEEMES', 'mutation_seq': 'MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKTGLRVAVKKLSRPFQSIIHAKRTYRELRLLKHMKHENVIGLLDVFTPARSLEEFNDVYLVTHLMGADLNNIVKCQKLTDDHVQFLIYQILRGLKYIHSADIIHRDLKPSNLAVNEDCELKILDFGLARHADDEMTGYVATRWYRAPEIMLNWMHYNQTVDIWSVGCIMAELLTGRTLFPGTDHIDQLKLILRLVGTPGAELLKKISSESARNYIQSLTQMPKMNFANVFIGANPLAVDLLEKMLVLDSDKRITAAQALAHAYFAQYHDPDDEPVADPYDQSFESRDLLIDEWKSLTYDEVISFVPPPLDQEEMES', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'No effect on either the kinase activity or tyrosine phosphorylation.', 'id': 'Q16539-T175A'}
[DEBUG] processed_record: {'wild_type_seq': 'MAFMKKYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGIGREMAYHLAKMGAHVVVTARSKETLQKVVSHCLELGAASAHYIAGTMEDMTFAEQFVAQAGKLMGGLDMLILNHITNTSLNLFHDDIHHVRKSMEVNFLSYVVLTVAALPMLKQSNGSIVVVSSLAGKVAYPMVAAYSASKFALDGFFSSIRKEYSVSRVNVSITLCVLGLIDTETAMKAVSGIVHMQAAPKEECALEIIKGGALRQEEVYYDSSLWTTLLIRNPCRKILEFLYSTSYNMDRFINK', 'mutation_seq': 'MAFMKSYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGIGREMAYHLAKMGAHVVVTARSKETLQKVVSHCLELGAASAHYIAGTMEDMTFAEQFVAQAGKLMGGLDMLILNHITNTSLNLFHDDIHHVRKSMEVNFLSYVVLTVAALPMLKQSNGSIVVVSSLAGKVAYPMVAAYSASKFALDGFFSSIRKEYSVSRVNVSITLCVLGLIDTETAMKAVSGIVHMQAAPKEECALEIIKGGALRQEEVYYDSSLWTTLLIRNPCRKILEFLYSTSYNMDRFINK', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.", 'gpt_response': 'No effect on topology or activity. Mutation of K(6) to S did not affect the subcellular localization or catalytic activity of 11beta-HSD1.', 'id': 'P28845-K6S'}
[DEBUG] processed_record: {'wild_type_seq': 'MAPLHHILVLCVGFLTTATAEAPQEHDPFTYDYQSLRIGGLIIAGILFILGILIVLSRRCRCKFNQQQRTGEPDEEEGTFRSSIRRLSTRRR', 'mutation_seq': None, 'human_query': '<wt_protein> </wt_protein> Describe this protein?', 'gpt_response': 'Loss of glutathionylation and loss of ability to reduce glutathionylation of ATP1B1. The mutation in the FXYD protein family affects the reactivity of the Na(+)-K(+) pump and modulates its activity.', 'id': 'P56513-K63G'}
[DEBUG] processed_record: {'wild_type_seq': 'MGLFNAHAVAQQRADRIATLLQSFADGQLDTAVGEAPAPGYERLYDSLRALQRQLREQRAELQQVESLEAGLAEMSRQHEAGWIDQTIPAERLEGRAARIAKGVNELVAAHIAVKMKVVSVVTAYGQGNFEPLMDRLPGKKAQITEAIDGVRERLRGAAEATSAQLATAAYNARIKSALDNVSANVMIADNDLNIIYMNRTVSEMLGRAEADIRKQLPNFDAGRLMGANIDVFHKNPAHQRHLLANLTGVHKAELNLGGRRFSLDVVPVFNDANERLGSAVQWTDRTEEHRAEQEVSQLVQAAAAGDFSKRVEEAGKEGFFLRLAKDLNSLVDTADRGLRDVSRMLGALAQGDLTQRIEADYQGTFGQLKDFSNDTAQSLSRMLGQIREAADTINTAASEIASGNAELSARTEQQASSLEETASSMEELTSTVKLNAENARQANSLAANASEVATQGGTVVQKVVSTMSSINESARKIADIIGVIDGIAFQTNILALNAAVEAARAGEQGRGFAVVAGEVRTLAQRSAAAAKEIKTLISDSVDKVENGNTLVAQAGQTMSDIVVAIRRVTDIMSEIAAASAEQSTGIEEVNSAVSQMDDMTQQNAALVEEAAAAAEAMQEQAGLLNQSVAVFRLDTPPSVVQLASARPSAPRPSAPAPLARSGMARASKARKEDGWEEF', 'mutation_seq': 'MGLFNAHAVAQQRADRIATLLQSFADGQLDTAVGEAPAPGYERLYDSLRALQRQLREQRAELQQVESLEAGLAEMSRQHEAGWIDQTIPAERLEGRAARIAKGVNELVAAHIAVKMKVVSVVTAYGQGNFEPLMDRLPGKKAQITEAIDGVRERLRGAAEATSAQLATAAYNARIKSALDNVSANVMIADNDLNIIYMNRTVSEMLGRAEADIRKQLPNFDAGRLMGANIDVFHKNPAHQRHLLANLTGVHKAELNLGGRRFSKDVVPVFNDANERLGSAVQWTDRTEEHRAEQEVSQLVQAAAAGDFSKRVEEAGKEGFFLRLAKDLNSLVDTADRGLRDVSRMLGALAQGDLTQRIEADYQGTFGQLKDFSNDTAQSLSRMLGQIREAADTINTAASEIASGNAELSARTEQQASSLEETASSMEELTSTVKLNAENARQANSLAANASEVATQGGTVVQKVVSTMSSINESARKIADIIGVIDGIAFQTNILALNAAVEAARAGEQGRGFAVVAGEVRTLAQRSAAAAKEIKTLISDSVDKVENGNTLVAQAGQTMSDIVVAIRRVTDIMSEIAAASAEQSTGIEEVNSAVSQMDDMTQQNAALVEEAAAAAEAMQEQAGLLNQSVAVFRLDTPPSVVQLASARPSAPRPSAPAPLARSGMARASKARKEDGWEEF', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.", 'gpt_response': 'Signal-off mutant that does not respond to the addition or removal of O(2). Does not bind either O(2) or CO. formed a hexacoordinate heme.', 'id': 'Q9I6V6-L264K'}
[DEBUG] processed_record: {'wild_type_seq': 'MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSLHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL', 'mutation_seq': 'MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSFHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'In BCHED; seems to cause reduced expression of the protein. The mutation in the BCHE gene leads to reduced expression of BChE protein.', 'id': 'P06276-L153F'}
[DEBUG] processed_record: {'wild_type_seq': 'MGCTLSAEDKAAVERSKMIDRNLREDGEKAAREVKLLLLGAGESGKSTIVKQMKIIHEAGYSEEECKQYKAVVYSNTIQSIIAIIRAMGRLKIDFGDSARADDARQLFVLAGAAEEGFMTAELAGVIKRLWKDSGVQACFNRSREYQLNDSAAYYLNDLDRIAQPNYIPTQQDVLRTRVKTTGIVETHFTFKDLHFKMFDVGGQRSERKKWIHCFEGVTAIIFCVALSDYDLVLAEDEEMNRMHESMKLFDSICNNKWFTDTSIILFLNKKDLFEEKIKKSPLTICYPEYAGSNTYEEAAAYIQCQFEDLNKRKDTKEIYTHFTCATDTKNVQFVFDAVTDVIIKNNLKDCGLF', 'mutation_seq': None, 'human_query': '<wt_protein> </wt_protein> Describe this protein?', 'gpt_response': 'Enhances interaction (inactive GDP-bound) with RGS14.', 'id': 'P63096-E245L'}
[DEBUG] processed_record: {'wild_type_seq': 'MPGGLLLGDEAPNFEANTTIGHIRFHDFLGDSWGILFSHPRDFTPVCTTELGRAAKLAPEFAKRNVKLIALSIDSVEDHFAWSKDINAYNGAAPTEKLPFPIIDDKDRDLAILLGMLDPAEKDEKGMPVTARVVFIFGPDKKLKLSILYPATTGRNFDEILRVVDSLQLTASNPVATPVDWKKGESVMVLPTLPEEEAKQLFPKGVFTKELPSGKKYLRYTPQP', 'mutation_seq': 'MPGGLLLGDEAPNFEANTTIGHIRFHDFLGDAWGILFSHPRDFTPVCTTELGRAAKLAPEFAKRNVKLIALSIDSVEDHFAWSKDINAYNGAAPTEKLPFPIIDDKDRDLAILLGMLDPAEKDEKGMPVTARVVFIFGPDKKLKLSILYPATTGRNFDEILRVVDSLQLTASNPVATPVDWKKGESVMVLPTLPEEEAKQLFPKGVFTKELPSGKKYLRYTPQP', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'Abolishes lipid binding. Increased Prdx6 alpha-helical content, key role in maintaining Prdx6 structure and for phospholipid substrate binding.', 'id': 'O35244-S32A'}
[DEBUG] processed_record: {'wild_type_seq': 'MERWPWPSGGAWLLVAARALLQLLRSDLRLGRPLLAALALLAALDWLCQRLLPPPAALAVLAAAGWIALSRLARPQRLPVATRAVLITGCDSGFGKETAKKLDSMGFTVLATVLELNSPGAIELRTCCSPRLRLLQMDLTKPGDISRVLEFTKAHTTSTGLWGLVNNAGHNEVVADAELSPVATFRSCMEVNFFGALELTKGLLPLLRSSRGRIVTVGSPAGDMPYPCLGAYGTSKAAVALLMDTFSCELLPWGVKVSIIQPGCFKTESVRNVGQWEKRKQLLLANLPQELLQAYGKDYIEHLHGQFLHSLRLAMSDLTPVVDAITDALLAARPRRRYYPGQGLGLMYFIHYYLPEGLRRRFLQAFFISHCLPRALQPGQPGTTPPQDAAQDPNLSPGPSPAVAR', 'mutation_seq': 'MERWPWPSGGAWLLVAARALLQLLRSDLRLGRPLLAALALLAALDWLCQRLLPPPAALAVLAAAGWIALSRLARPQRLPVATRAVLITGCDSGFGKETAKKLDSMGFTVLATVLELNSPGAIELRTCCSPRLRLLQMDLTKPGDISRVLEFTKAHTTSTGLWGLVNNAGHNEVVADAELSPVATFRSCMEVNFFGALELTKGLLPLLRSSRGRIVTVGSPAGDMPYPCLGAYGTSKAAVALLMDTFSCELLPWGVKVSIIQPGCFKTESVRNVGQWEKRKQLLLANLPQELLQAYGKDYIEHLHGQFLHSLRLAMSDLTPVVDAITDALLAARPRRCYYPGQGLGLMYFIHYYLPEGLRRRFLQAFFISHCLPRALQPGQPGTTPPQDAAQDPNLSPGPSPAVAR', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.", 'gpt_response': 'In AME; decreased half-life from 21 to 4 hours compared to wild-type, probably due to degradation via the proteasomal pathway. This mutation has been discovered in a consanguineous Iranian family with three siblings suffering from Apparent Mineralocorticoid Excess (AME). The mutation in the HSD11B2 gene is associated with apparent mineralocorticoid excess (AME). This mutation results in a deficiency of 11 beta-hydroxysteroid dehydrogenase type 2 enzyme activity, leading to pre- and postnatal growth failure, juvenile hypertension, hypokalemic metabolic alkalosis, and hyporeninemic hypoaldosteronism. The 11 beta HSD2 enzyme is responsible for the conversion of cortisol to the inactive metabolite cortisone, and the mutation impairs this conversion, causing mineralocorticoid receptors to be exposed to excessive cortisol levels.', 'id': 'P80365-R337C'}
[DEBUG] processed_record: {'wild_type_seq': 'MLTCEMRESALARLGRALADPTRCRILVALLDGVCYPGQLAAHLGLTRSNVSNHLSCLRGCGLVVATYEGRQVRYALADSHLARALGELVQVVLAVDTDQPCVAERAASGEAVEMTGS', 'mutation_seq': 'MLTSEMRESALARLGRALADPTRCRILVALLDGVCYPGQLAAHLGLTRSNVSNHLSCLRGCGLVVATYEGRQVRYALADSHLARALGELVQVVLAVDTDQPCVAERAASGEAVEMTGS', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.", 'gpt_response': 'No effect. Substitution of C4 with S is obligatory for Cd(II) detection.', 'id': 'P9WMI9-C4S'}
[DEBUG] processed_record: {'wild_type_seq': 'MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKTGLRVAVKKLSRPFQSIIHAKRTYRELRLLKHMKHENVIGLLDVFTPARSLEEFNDVYLVTHLMGADLNNIVKCQKLTDDHVQFLIYQILRGLKYIHSADIIHRDLKPSNLAVNEDCELKILDFGLARHTDDEMTGYVATRWYRAPEIMLNWMHYNQTVDIWSVGCIMAELLTGRTLFPGTDHIDQLKLILRLVGTPGAELLKKISSESARNYIQSLTQMPKMNFANVFIGANPLAVDLLEKMLVLDSDKRITAAQALAHAYFAQYHDPDDEPVADPYDQSFESRDLLIDEWKSLTYDEVISFVPPPLDQEEMES', 'mutation_seq': 'MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKTGLRVAVKKLSRPFQSIIHAKRTYRELRLLKHMKHENVIGLLDVFTPARSLEEFNDVYLVTHLMGADLNNIVKCQKLTDDHVQFLIYQILRGLKYIHSADIIHRDLKPSNLAVNEDCELKILDFGLARHADDEMTGYVATRWYRAPEIMLNWMHYNQTVDIWSVGCIMAELLTGRTLFPGTDHIDQLKLILRLVGTPGAELLKKISSESARNYIQSLTQMPKMNFANVFIGANPLAVDLLEKMLVLDSDKRITAAQALAHAYFAQYHDPDDEPVADPYDQSFESRDLLIDEWKSLTYDEVISFVPPPLDQEEMES', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'No effect on either the kinase activity or tyrosine phosphorylation.', 'id': 'Q16539-T175A'}
[DEBUG] processed_record: {'wild_type_seq': 'MAFMKKYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGIGREMAYHLAKMGAHVVVTARSKETLQKVVSHCLELGAASAHYIAGTMEDMTFAEQFVAQAGKLMGGLDMLILNHITNTSLNLFHDDIHHVRKSMEVNFLSYVVLTVAALPMLKQSNGSIVVVSSLAGKVAYPMVAAYSASKFALDGFFSSIRKEYSVSRVNVSITLCVLGLIDTETAMKAVSGIVHMQAAPKEECALEIIKGGALRQEEVYYDSSLWTTLLIRNPCRKILEFLYSTSYNMDRFINK', 'mutation_seq': 'MAFMKSYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGIGREMAYHLAKMGAHVVVTARSKETLQKVVSHCLELGAASAHYIAGTMEDMTFAEQFVAQAGKLMGGLDMLILNHITNTSLNLFHDDIHHVRKSMEVNFLSYVVLTVAALPMLKQSNGSIVVVSSLAGKVAYPMVAAYSASKFALDGFFSSIRKEYSVSRVNVSITLCVLGLIDTETAMKAVSGIVHMQAAPKEECALEIIKGGALRQEEVYYDSSLWTTLLIRNPCRKILEFLYSTSYNMDRFINK', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.", 'gpt_response': 'No effect on topology or activity. Mutation of K(6) to S did not affect the subcellular localization or catalytic activity of 11beta-HSD1.', 'id': 'P28845-K6S'}
[DEBUG] processed_record: {'wild_type_seq': 'MAPLHHILVLCVGFLTTATAEAPQEHDPFTYDYQSLRIGGLIIAGILFILGILIVLSRRCRCKFNQQQRTGEPDEEEGTFRSSIRRLSTRRR', 'mutation_seq': None, 'human_query': '<wt_protein> </wt_protein> Describe this protein?', 'gpt_response': 'Loss of glutathionylation and loss of ability to reduce glutathionylation of ATP1B1. The mutation in the FXYD protein family affects the reactivity of the Na(+)-K(+) pump and modulates its activity.', 'id': 'P56513-K63G'}
[DEBUG] processed_record: {'wild_type_seq': 'MGLFNAHAVAQQRADRIATLLQSFADGQLDTAVGEAPAPGYERLYDSLRALQRQLREQRAELQQVESLEAGLAEMSRQHEAGWIDQTIPAERLEGRAARIAKGVNELVAAHIAVKMKVVSVVTAYGQGNFEPLMDRLPGKKAQITEAIDGVRERLRGAAEATSAQLATAAYNARIKSALDNVSANVMIADNDLNIIYMNRTVSEMLGRAEADIRKQLPNFDAGRLMGANIDVFHKNPAHQRHLLANLTGVHKAELNLGGRRFSLDVVPVFNDANERLGSAVQWTDRTEEHRAEQEVSQLVQAAAAGDFSKRVEEAGKEGFFLRLAKDLNSLVDTADRGLRDVSRMLGALAQGDLTQRIEADYQGTFGQLKDFSNDTAQSLSRMLGQIREAADTINTAASEIASGNAELSARTEQQASSLEETASSMEELTSTVKLNAENARQANSLAANASEVATQGGTVVQKVVSTMSSINESARKIADIIGVIDGIAFQTNILALNAAVEAARAGEQGRGFAVVAGEVRTLAQRSAAAAKEIKTLISDSVDKVENGNTLVAQAGQTMSDIVVAIRRVTDIMSEIAAASAEQSTGIEEVNSAVSQMDDMTQQNAALVEEAAAAAEAMQEQAGLLNQSVAVFRLDTPPSVVQLASARPSAPRPSAPAPLARSGMARASKARKEDGWEEF', 'mutation_seq': 'MGLFNAHAVAQQRADRIATLLQSFADGQLDTAVGEAPAPGYERLYDSLRALQRQLREQRAELQQVESLEAGLAEMSRQHEAGWIDQTIPAERLEGRAARIAKGVNELVAAHIAVKMKVVSVVTAYGQGNFEPLMDRLPGKKAQITEAIDGVRERLRGAAEATSAQLATAAYNARIKSALDNVSANVMIADNDLNIIYMNRTVSEMLGRAEADIRKQLPNFDAGRLMGANIDVFHKNPAHQRHLLANLTGVHKAELNLGGRRFSKDVVPVFNDANERLGSAVQWTDRTEEHRAEQEVSQLVQAAAAGDFSKRVEEAGKEGFFLRLAKDLNSLVDTADRGLRDVSRMLGALAQGDLTQRIEADYQGTFGQLKDFSNDTAQSLSRMLGQIREAADTINTAASEIASGNAELSARTEQQASSLEETASSMEELTSTVKLNAENARQANSLAANASEVATQGGTVVQKVVSTMSSINESARKIADIIGVIDGIAFQTNILALNAAVEAARAGEQGRGFAVVAGEVRTLAQRSAAAAKEIKTLISDSVDKVENGNTLVAQAGQTMSDIVVAIRRVTDIMSEIAAASAEQSTGIEEVNSAVSQMDDMTQQNAALVEEAAAAAEAMQEQAGLLNQSVAVFRLDTPPSVVQLASARPSAPRPSAPAPLARSGMARASKARKEDGWEEF', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.", 'gpt_response': 'Signal-off mutant that does not respond to the addition or removal of O(2). Does not bind either O(2) or CO. formed a hexacoordinate heme.', 'id': 'Q9I6V6-L264K'}
[DEBUG] processed_record: {'wild_type_seq': 'MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSLHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL', 'mutation_seq': 'MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSFHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'In BCHED; seems to cause reduced expression of the protein. The mutation in the BCHE gene leads to reduced expression of BChE protein.', 'id': 'P06276-L153F'}
[DEBUG] processed_record: {'wild_type_seq': 'MGCTLSAEDKAAVERSKMIDRNLREDGEKAAREVKLLLLGAGESGKSTIVKQMKIIHEAGYSEEECKQYKAVVYSNTIQSIIAIIRAMGRLKIDFGDSARADDARQLFVLAGAAEEGFMTAELAGVIKRLWKDSGVQACFNRSREYQLNDSAAYYLNDLDRIAQPNYIPTQQDVLRTRVKTTGIVETHFTFKDLHFKMFDVGGQRSERKKWIHCFEGVTAIIFCVALSDYDLVLAEDEEMNRMHESMKLFDSICNNKWFTDTSIILFLNKKDLFEEKIKKSPLTICYPEYAGSNTYEEAAAYIQCQFEDLNKRKDTKEIYTHFTCATDTKNVQFVFDAVTDVIIKNNLKDCGLF', 'mutation_seq': None, 'human_query': '<wt_protein> </wt_protein> Describe this protein?', 'gpt_response': 'Enhances interaction (inactive GDP-bound) with RGS14.', 'id': 'P63096-E245L'}
[DEBUG] processed_record: {'wild_type_seq': 'MPGGLLLGDEAPNFEANTTIGHIRFHDFLGDSWGILFSHPRDFTPVCTTELGRAAKLAPEFAKRNVKLIALSIDSVEDHFAWSKDINAYNGAAPTEKLPFPIIDDKDRDLAILLGMLDPAEKDEKGMPVTARVVFIFGPDKKLKLSILYPATTGRNFDEILRVVDSLQLTASNPVATPVDWKKGESVMVLPTLPEEEAKQLFPKGVFTKELPSGKKYLRYTPQP', 'mutation_seq': 'MPGGLLLGDEAPNFEANTTIGHIRFHDFLGDAWGILFSHPRDFTPVCTTELGRAAKLAPEFAKRNVKLIALSIDSVEDHFAWSKDINAYNGAAPTEKLPFPIIDDKDRDLAILLGMLDPAEKDEKGMPVTARVVFIFGPDKKLKLSILYPATTGRNFDEILRVVDSLQLTASNPVATPVDWKKGESVMVLPTLPEEEAKQLFPKGVFTKELPSGKKYLRYTPQP', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'Abolishes lipid binding. Increased Prdx6 alpha-helical content, key role in maintaining Prdx6 structure and for phospholipid substrate binding.', 'id': 'O35244-S32A'}
[DEBUG] processed_record: {'wild_type_seq': 'MERWPWPSGGAWLLVAARALLQLLRSDLRLGRPLLAALALLAALDWLCQRLLPPPAALAVLAAAGWIALSRLARPQRLPVATRAVLITGCDSGFGKETAKKLDSMGFTVLATVLELNSPGAIELRTCCSPRLRLLQMDLTKPGDISRVLEFTKAHTTSTGLWGLVNNAGHNEVVADAELSPVATFRSCMEVNFFGALELTKGLLPLLRSSRGRIVTVGSPAGDMPYPCLGAYGTSKAAVALLMDTFSCELLPWGVKVSIIQPGCFKTESVRNVGQWEKRKQLLLANLPQELLQAYGKDYIEHLHGQFLHSLRLAMSDLTPVVDAITDALLAARPRRRYYPGQGLGLMYFIHYYLPEGLRRRFLQAFFISHCLPRALQPGQPGTTPPQDAAQDPNLSPGPSPAVAR', 'mutation_seq': 'MERWPWPSGGAWLLVAARALLQLLRSDLRLGRPLLAALALLAALDWLCQRLLPPPAALAVLAAAGWIALSRLARPQRLPVATRAVLITGCDSGFGKETAKKLDSMGFTVLATVLELNSPGAIELRTCCSPRLRLLQMDLTKPGDISRVLEFTKAHTTSTGLWGLVNNAGHNEVVADAELSPVATFRSCMEVNFFGALELTKGLLPLLRSSRGRIVTVGSPAGDMPYPCLGAYGTSKAAVALLMDTFSCELLPWGVKVSIIQPGCFKTESVRNVGQWEKRKQLLLANLPQELLQAYGKDYIEHLHGQFLHSLRLAMSDLTPVVDAITDALLAARPRRCYYPGQGLGLMYFIHYYLPEGLRRRFLQAFFISHCLPRALQPGQPGTTPPQDAAQDPNLSPGPSPAVAR', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.", 'gpt_response': 'In AME; decreased half-life from 21 to 4 hours compared to wild-type, probably due to degradation via the proteasomal pathway. This mutation has been discovered in a consanguineous Iranian family with three siblings suffering from Apparent Mineralocorticoid Excess (AME). The mutation in the HSD11B2 gene is associated with apparent mineralocorticoid excess (AME). This mutation results in a deficiency of 11 beta-hydroxysteroid dehydrogenase type 2 enzyme activity, leading to pre- and postnatal growth failure, juvenile hypertension, hypokalemic metabolic alkalosis, and hyporeninemic hypoaldosteronism. The 11 beta HSD2 enzyme is responsible for the conversion of cortisol to the inactive metabolite cortisone, and the mutation impairs this conversion, causing mineralocorticoid receptors to be exposed to excessive cortisol levels.', 'id': 'P80365-R337C'}
[DEBUG] processed_record: {'wild_type_seq': 'MLTCEMRESALARLGRALADPTRCRILVALLDGVCYPGQLAAHLGLTRSNVSNHLSCLRGCGLVVATYEGRQVRYALADSHLARALGELVQVVLAVDTDQPCVAERAASGEAVEMTGS', 'mutation_seq': 'MLTSEMRESALARLGRALADPTRCRILVALLDGVCYPGQLAAHLGLTRSNVSNHLSCLRGCGLVVATYEGRQVRYALADSHLARALGELVQVVLAVDTDQPCVAERAASGEAVEMTGS', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.", 'gpt_response': 'No effect. Substitution of C4 with S is obligatory for Cd(II) detection.', 'id': 'P9WMI9-C4S'}
[DEBUG] processed_record: {'wild_type_seq': 'MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKTGLRVAVKKLSRPFQSIIHAKRTYRELRLLKHMKHENVIGLLDVFTPARSLEEFNDVYLVTHLMGADLNNIVKCQKLTDDHVQFLIYQILRGLKYIHSADIIHRDLKPSNLAVNEDCELKILDFGLARHTDDEMTGYVATRWYRAPEIMLNWMHYNQTVDIWSVGCIMAELLTGRTLFPGTDHIDQLKLILRLVGTPGAELLKKISSESARNYIQSLTQMPKMNFANVFIGANPLAVDLLEKMLVLDSDKRITAAQALAHAYFAQYHDPDDEPVADPYDQSFESRDLLIDEWKSLTYDEVISFVPPPLDQEEMES', 'mutation_seq': 'MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKTGLRVAVKKLSRPFQSIIHAKRTYRELRLLKHMKHENVIGLLDVFTPARSLEEFNDVYLVTHLMGADLNNIVKCQKLTDDHVQFLIYQILRGLKYIHSADIIHRDLKPSNLAVNEDCELKILDFGLARHADDEMTGYVATRWYRAPEIMLNWMHYNQTVDIWSVGCIMAELLTGRTLFPGTDHIDQLKLILRLVGTPGAELLKKISSESARNYIQSLTQMPKMNFANVFIGANPLAVDLLEKMLVLDSDKRITAAQALAHAYFAQYHDPDDEPVADPYDQSFESRDLLIDEWKSLTYDEVISFVPPPLDQEEMES', 'human_query': '<mut_protein> </mut_protein> Describe the effect of the mutation?', 'gpt_response': 'No effect on either the kinase activity or tyrosine phosphorylation.', 'id': 'Q16539-T175A'}
[DEBUG] processed_record: {'wild_type_seq': 'MAFMKKYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGIGREMAYHLAKMGAHVVVTARSKETLQKVVSHCLELGAASAHYIAGTMEDMTFAEQFVAQAGKLMGGLDMLILNHITNTSLNLFHDDIHHVRKSMEVNFLSYVVLTVAALPMLKQSNGSIVVVSSLAGKVAYPMVAAYSASKFALDGFFSSIRKEYSVSRVNVSITLCVLGLIDTETAMKAVSGIVHMQAAPKEECALEIIKGGALRQEEVYYDSSLWTTLLIRNPCRKILEFLYSTSYNMDRFINK', 'mutation_seq': 'MAFMKSYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGIGREMAYHLAKMGAHVVVTARSKETLQKVVSHCLELGAASAHYIAGTMEDMTFAEQFVAQAGKLMGGLDMLILNHITNTSLNLFHDDIHHVRKSMEVNFLSYVVLTVAALPMLKQSNGSIVVVSSLAGKVAYPMVAAYSASKFALDGFFSSIRKEYSVSRVNVSITLCVLGLIDTETAMKAVSGIVHMQAAPKEECALEIIKGGALRQEEVYYDSSLWTTLLIRNPCRKILEFLYSTSYNMDRFINK', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.", 'gpt_response': 'No effect on topology or activity. Mutation of K(6) to S did not affect the subcellular localization or catalytic activity of 11beta-HSD1.', 'id': 'P28845-K6S'}
[DEBUG] processed_record: {'wild_type_seq': 'MAPLHHILVLCVGFLTTATAEAPQEHDPFTYDYQSLRIGGLIIAGILFILGILIVLSRRCRCKFNQQQRTGEPDEEEGTFRSSIRRLSTRRR', 'mutation_seq': None, 'human_query': '<wt_protein> </wt_protein> Describe this protein?', 'gpt_response': 'Loss of glutathionylation and loss of ability to reduce glutathionylation of ATP1B1. The mutation in the FXYD protein family affects the reactivity of the Na(+)-K(+) pump and modulates its activity.', 'id': 'P56513-K63G'}
[DEBUG] processed_record: {'wild_type_seq': 'MGLFNAHAVAQQRADRIATLLQSFADGQLDTAVGEAPAPGYERLYDSLRALQRQLREQRAELQQVESLEAGLAEMSRQHEAGWIDQTIPAERLEGRAARIAKGVNELVAAHIAVKMKVVSVVTAYGQGNFEPLMDRLPGKKAQITEAIDGVRERLRGAAEATSAQLATAAYNARIKSALDNVSANVMIADNDLNIIYMNRTVSEMLGRAEADIRKQLPNFDAGRLMGANIDVFHKNPAHQRHLLANLTGVHKAELNLGGRRFSLDVVPVFNDANERLGSAVQWTDRTEEHRAEQEVSQLVQAAAAGDFSKRVEEAGKEGFFLRLAKDLNSLVDTADRGLRDVSRMLGALAQGDLTQRIEADYQGTFGQLKDFSNDTAQSLSRMLGQIREAADTINTAASEIASGNAELSARTEQQASSLEETASSMEELTSTVKLNAENARQANSLAANASEVATQGGTVVQKVVSTMSSINESARKIADIIGVIDGIAFQTNILALNAAVEAARAGEQGRGFAVVAGEVRTLAQRSAAAAKEIKTLISDSVDKVENGNTLVAQAGQTMSDIVVAIRRVTDIMSEIAAASAEQSTGIEEVNSAVSQMDDMTQQNAALVEEAAAAAEAMQEQAGLLNQSVAVFRLDTPPSVVQLASARPSAPRPSAPAPLARSGMARASKARKEDGWEEF', 'mutation_seq': 'MGLFNAHAVAQQRADRIATLLQSFADGQLDTAVGEAPAPGYERLYDSLRALQRQLREQRAELQQVESLEAGLAEMSRQHEAGWIDQTIPAERLEGRAARIAKGVNELVAAHIAVKMKVVSVVTAYGQGNFEPLMDRLPGKKAQITEAIDGVRERLRGAAEATSAQLATAAYNARIKSALDNVSANVMIADNDLNIIYMNRTVSEMLGRAEADIRKQLPNFDAGRLMGANIDVFHKNPAHQRHLLANLTGVHKAELNLGGRRFSKDVVPVFNDANERLGSAVQWTDRTEEHRAEQEVSQLVQAAAAGDFSKRVEEAGKEGFFLRLAKDLNSLVDTADRGLRDVSRMLGALAQGDLTQRIEADYQGTFGQLKDFSNDTAQSLSRMLGQIREAADTINTAASEIASGNAELSARTEQQASSLEETASSMEELTSTVKLNAENARQANSLAANASEVATQGGTVVQKVVSTMSSINESARKIADIIGVIDGIAFQTNILALNAAVEAARAGEQGRGFAVVAGEVRTLAQRSAAAAKEIKTLISDSVDKVENGNTLVAQAGQTMSDIVVAIRRVTDIMSEIAAASAEQSTGIEEVNSAVSQMDDMTQQNAALVEEAAAAAEAMQEQAGLLNQSVAVFRLDTPPSVVQLASARPSAPRPSAPAPLARSGMARASKARKEDGWEEF', 'human_query': "Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.", 'gpt_response': 'Signal-off mutant that does not respond to the addition or removal of O(2). Does not bind either O(2) or CO. formed a hexacoordinate heme.', 'id': 'Q9I6V6-L264K'}
[DEBUG] Dataset initialized with:
[DEBUG] - pad_token: <|eot_id|>
[DEBUG] - pad_token_id: 128009
[DEBUG] - max_text_len: 1536
[DEBUG] - num_samples: 37
[DEBUG] Training dataset initialized. Number of samples: 37
[TIME] Dataset initialization took 0.00 seconds.

======= TRAINABLE PARAMETERS VALIDATION =======
Total number of trainable parameters: 307
Sample trainable parameter names:
  - base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight

Adapter parameters that are trainable: 51
  - base_model.model.mm_gated_cross_attention.ff_gate
  - base_model.model.mm_gated_cross_attention.norm_query.weight
  - base_model.model.mm_gated_cross_attention.norm_query.bias
  - base_model.model.mm_gated_cross_attention.norm_key_value.weight
  - base_model.model.mm_gated_cross_attention.norm_key_value.bias

LoRA parameters that are trainable: 256
  - base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
===============================================

[DEBUG] AdapterTrainer initialized.
[TIME] Trainer initialization took 0.01 seconds.

=== DEBUG: Verifying Model State Before Training ===
Trainable parameters:
  - base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight
  - base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
  - base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
  - base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight
  - base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight
  - base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
  - base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
  - base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight
  - base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight
  - base_model.model.mm_gated_cross_attention.ff_gate
  - base_model.model.mm_gated_cross_attention.norm_query.weight
  - base_model.model.mm_gated_cross_attention.norm_query.bias
  - base_model.model.mm_gated_cross_attention.norm_key_value.weight
  - base_model.model.mm_gated_cross_attention.norm_key_value.bias
  - base_model.model.mm_gated_cross_attention.to_q_cross.weight
  - base_model.model.mm_gated_cross_attention.to_kv_cross.weight
  - base_model.model.mm_gated_cross_attention.to_qkv_self_attn.weight
  - base_model.model.mm_gated_cross_attention.gate_norm.weight
  - base_model.model.mm_gated_cross_attention.gate_norm.bias
  - base_model.model.mm_gated_cross_attention.gate_attention.in_proj_weight
  - base_model.model.mm_gated_cross_attention.gate_attention.out_proj.weight
  - base_model.model.mm_gated_cross_attention.to_mixing_weights.0.weight
  - base_model.model.mm_gated_cross_attention.to_mixing_weights.0.bias
  - base_model.model.mm_gated_cross_attention.to_out.weight
  - base_model.model.mm_gated_cross_attention.ff.norm.weight
  - base_model.model.mm_gated_cross_attention.ff.norm.bias
  - base_model.model.mm_gated_cross_attention.ff.fc1.weight
  - base_model.model.mm_gated_cross_attention.ff.fc2.weight
  - base_model.model.mm_resampler.latents
  - base_model.model.mm_resampler.latent_pos_emb
  - base_model.model.mm_resampler.layers.0.0.norm_media.weight
  - base_model.model.mm_resampler.layers.0.0.norm_media.bias
  - base_model.model.mm_resampler.layers.0.0.norm_latents.weight
  - base_model.model.mm_resampler.layers.0.0.norm_latents.bias
  - base_model.model.mm_resampler.layers.0.0.to_q.weight
  - base_model.model.mm_resampler.layers.0.0.to_kv.weight
  - base_model.model.mm_resampler.layers.0.0.to_out.weight
  - base_model.model.mm_resampler.layers.0.1.norm.weight
  - base_model.model.mm_resampler.layers.0.1.norm.bias
  - base_model.model.mm_resampler.layers.0.1.fc1.weight
  - base_model.model.mm_resampler.layers.0.1.fc2.weight
  - base_model.model.mm_resampler.layers.1.0.norm_media.weight
  - base_model.model.mm_resampler.layers.1.0.norm_media.bias
  - base_model.model.mm_resampler.layers.1.0.norm_latents.weight
  - base_model.model.mm_resampler.layers.1.0.norm_latents.bias
  - base_model.model.mm_resampler.layers.1.0.to_q.weight
  - base_model.model.mm_resampler.layers.1.0.to_kv.weight
  - base_model.model.mm_resampler.layers.1.0.to_out.weight
  - base_model.model.mm_resampler.layers.1.1.norm.weight
  - base_model.model.mm_resampler.layers.1.1.norm.bias
  - base_model.model.mm_resampler.layers.1.1.fc1.weight
  - base_model.model.mm_resampler.layers.1.1.fc2.weight
  - base_model.model.mm_resampler.norm.weight
  - base_model.model.mm_resampler.norm.bias
  - base_model.model.mm_resampler.proj_out.weight
  - base_model.model.mm_resampler.proj_out.bias
  - base_model.model.mm_projector.linear1.weight
  - base_model.model.mm_projector.linear1.bias
  - base_model.model.mm_projector.linear2.weight
  - base_model.model.mm_projector.linear2.bias
================================================

[DEBUG] Starting LoRA finetuning...
[DEBUG] train_dataset length: 37
[DEBUG] training_args.num_train_epochs: 5
[DEBUG] training_args.per_device_train_batch_size: 4
[DEBUG] training_args.gradient_accumulation_steps: 1
Installed CUDA version 12.4 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.628286361694336 seconds
[2025-07-04 21:02:57,858] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[DEBUG] Sample 23:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 2:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 32:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 18:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 17:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 34:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 16:[DEBUG] Sample 26:
[DEBUG] Sample 25:
  - has_delta: True
  - has_delta: False
  - has_delta: True
  - has_wt: True
  - has_wt: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] attention_mode: wt_only
[DEBUG] attention_mode: full

[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 28:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 30:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] Sample 36:[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 7:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 19:[DEBUG] Sample 3:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 20:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 4:[DEBUG] Sample 9:[DEBUG] Sample 5:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full

[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.

  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 33:[DEBUG] Sample 31:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 11:[DEBUG] Sample 1:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 15:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 21:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 24:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 22:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] Sample 0:[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.

  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Aspartic acid to Alanine at position 95, the original function of the non-mutated protein is as follows: Nucleotidyltransferase that catalyzes the formation of cyclic GMP-AMP (2',3'-cGAMP) from ATP and GTP and plays a key role in innate immunity. Catalysis involves both the formation of a 2',5' phosphodiester linkage at the GpA step and the formation of a 3',5' phosphodiester linkage at the ApG step, producing c[G(2',5')pA(3',5')p]. Acts as a key DNA sensor: directly binds double-stranded DNA (dsDNA), inducing the formation of liquid-like droplets in which CGAS is activated, leading to synthesis of 2',3'-cGAMP, a second messenger that binds to and activates STING1, thereby triggering type-I interferon production. Preferentially recognizes and binds curved long dsDNAs of a minimal length of 40 bp. Acts as a key foreign DNA sensor, the presence of double-stranded DNA (dsDNA) in the cytoplasm being a danger signal that triggers the immune responses. Has antiviral activity by sensing the presence of dsDNA from DNA viruses in the cytoplasm. Also acts as an innate immune sensor of infection by retroviruses, such as HIV-2, by detecting the presence of reverse-transcribed DNA in the cytosol. In contrast, HIV-1 is poorly sensed by CGAS, due to its capsid that cloaks viral DNA from CGAS detection. Detection of retroviral reverse-transcribed DNA in the cytosol may be indirect and be mediated via interaction with PQBP1, which directly binds reverse-transcribed retroviral DNA. Also detects the presence of DNA from bacteria, such as M.tuberculosis. 2',3'-cGAMP can be transferred from producing cells to neighboring cells through gap junctions, leading to promote STING1 activation and convey immune response to connecting cells. 2',3'-cGAMP can also be transferred between cells by virtue of packaging within viral particles contributing to IFN-induction in newly infected cells in a cGAS-independent but STING1-dependent manner. Also senses the presence of neutrophil extracellular traps (NETs) that are translocated to the cytosol following phagocytosis, leading to synthesis of 2',3'-cGAMP. In addition to foreign DNA, can also be activated by endogenous nuclear or mitochondrial DNA. When self-DNA leaks into the cytosol during cellular stress (such as mitochondrial stress, SARS-CoV-2 infection causing severe COVID-19 disease, DNA damage, mitotic arrest or senescence), or is present in form of cytosolic micronuclei, CGAS is activated leading to a state of sterile inflammation. Acts as a regulator of cellular senescence by binding to cytosolic chromatin fragments that are present in senescent cells, leading to trigger type-I interferon production via STING1 and promote cellular senescence. Also involved in the inflammatory response to genome instability and double-stranded DNA breaks: acts by localizing to micronuclei arising from genome instability. Micronuclei, which are frequently found in cancer cells, consist of chromatin surrounded by their own nuclear membrane: following breakdown of the micronuclear envelope, a process associated with chromothripsis, CGAS binds self-DNA exposed to the cytosol, leading to 2',3'-cGAMP synthesis and subsequent activation of STING1 and type-I interferon production. Activated in response to prolonged mitotic arrest, promoting mitotic cell death. In a healthy cell, CGAS is however kept inactive even in cellular events that directly expose it to self-DNA, such as mitosis, when cGAS associates with chromatin directly after nuclear envelope breakdown or remains in the form of postmitotic persistent nuclear cGAS pools bound to chromatin. Nuclear CGAS is inactivated by chromatin via direct interaction with nucleosomes, which block CGAS from DNA binding and thus prevent CGAS-induced autoimmunity. Also acts as a suppressor of DNA repair in response to DNA damage: inhibits homologous recombination repair by interacting with PARP1, the CGAS-PARP1 interaction leading to impede the formation of the PARP1-TIMELESS complex. In addition to DNA, also sense translation stress: in response to translation stress, translocates to the cytosol and associates with collided ribosomes, promoting its activation and triggering type-I interferon production. In contrast to other mammals, human CGAS displays species-specific mechanisms of DNA recognition and produces less 2',3'-cGAMP, allowing a more fine-tuned response to pathogens.
[DEBUG] Sample 27:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 14:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 6:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 29:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 13:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full[DEBUG] Sample 35:

  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.

[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 8:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])[DEBUG] Sample 12:

  position_ids: None  - has_delta: True

  inputs_embeds: None  - has_wt: False

  labels: torch.Size([4, 1536])[DEBUG] attention_mode: delta_only


Sequence inputs:[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'wt_only', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
[DEBUG] Sample 10:
  full: 2 samples  - has_delta: True

  delta_only: 1 samples  - has_wt: False

[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -10752.0000
  - Mean value: -0.0736

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10880.0000
  - Mean value: -0.0766
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1072.0000
  - Min value: -11968.0000
  - Mean value: -0.2142

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1064.0000
  - Min value: -11904.0000
  - Mean value: -0.2144
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['wt_only', 'wt_only', 'full', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 0 samples
  wt_only: 3 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5105

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5045
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing wt_only group:
  Wild-type sequences: 3
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: 0.0079
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: None
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 2:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([3, 1551, 4096])
  attention_mask: torch.Size([3, 1551])
  labels: torch.Size([3, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])
  Padded shapes:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -11264.0000
  - Mean value: -0.1225

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -11264.0000
  - Mean value: -0.1237
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -11264.0000
  - Mean value: -0.1963

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -11264.0000
  - Mean value: -0.1957
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'full', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1954

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1980
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'delta_only', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 3 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -10752.0000
  - Mean value: -0.1480

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10880.0000
  - Mean value: -0.1513
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing delta_only group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 0

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([3, 1551, 4096])
  attention_mask: torch.Size([3, 1551])
  labels: torch.Size([3, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])
  Padded shapes:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['wt_only', 'delta_only', 'full', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 1 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1179

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1199
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: -0.0075
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'full', 'full', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 2 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1462

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1493
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1072.0000
  - Min value: -11968.0000
  - Mean value: -0.2142

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1064.0000
  - Min value: -11904.0000
  - Mean value: -0.2144
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['wt_only', 'wt_only', 'wt_only', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 0 samples
  delta_only: 1 samples
  wt_only: 3 samples
  text_only: 0 samples

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1072.0000
  - Min value: -11968.0000
  - Mean value: -0.2142

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1064.0000
  - Min value: -11904.0000
  - Mean value: -0.2144
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing wt_only group:
  Wild-type sequences: 3
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: 0.0233
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: None
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 2:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([3, 1551, 4096])
  attention_mask: torch.Size([3, 1551])
  labels: torch.Size([3, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1551
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Part 0 already at max length (1551)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])

Part 1 already at max length (1551)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1551])
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1551])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1551]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1551])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1551, 4096])
  attention_mask: torch.Size([4, 1551])
  labels: torch.Size([4, 1551])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1551])
  inputs_embeds: torch.Size([4, 1551, 4096])
  labels: torch.Size([4, 1551])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([1, 1536])

Sequence inputs:
  wild_type_sequences: 1
  mutation_sequences: 1
  attention_mode: ['delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 0 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1551
  Number of parts to process: 1

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Part 0 already at max length (1551)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1551])

Shapes after concatenation:
  Inputs embeds: torch.Size([1, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([1, 1551]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([1, 1551])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([1, 1551])
  inputs_embeds: torch.Size([1, 1551, 4096])
  labels: torch.Size([1, 1551])
=== End Forward Pass Debug ===
{'loss': 2.5131, 'learning_rate': 1e-05, 'epoch': 1.0}
[DEBUG] Sample 34:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 16:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 4:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 7:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 24:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 19:[DEBUG] Sample 26:

  - has_delta: True  - has_delta: False

  - has_wt: False  - has_wt: True

[DEBUG] attention_mode: delta_only[DEBUG] attention_mode: wt_only

[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?

[DEBUG] Sample 32:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 6:[DEBUG] Sample 8:

  - has_delta: True  - has_delta: False

  - has_wt: False  - has_wt: True

[DEBUG] attention_mode: delta_only[DEBUG] attention_mode: wt_only

[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?

[DEBUG] Sample 1:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 35:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 28:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 5:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 30:[DEBUG] Sample 36:[DEBUG] Sample 0:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Aspartic acid to Alanine at position 95, the original function of the non-mutated protein is as follows: Nucleotidyltransferase that catalyzes the formation of cyclic GMP-AMP (2',3'-cGAMP) from ATP and GTP and plays a key role in innate immunity. Catalysis involves both the formation of a 2',5' phosphodiester linkage at the GpA step and the formation of a 3',5' phosphodiester linkage at the ApG step, producing c[G(2',5')pA(3',5')p]. Acts as a key DNA sensor: directly binds double-stranded DNA (dsDNA), inducing the formation of liquid-like droplets in which CGAS is activated, leading to synthesis of 2',3'-cGAMP, a second messenger that binds to and activates STING1, thereby triggering type-I interferon production. Preferentially recognizes and binds curved long dsDNAs of a minimal length of 40 bp. Acts as a key foreign DNA sensor, the presence of double-stranded DNA (dsDNA) in the cytoplasm being a danger signal that triggers the immune responses. Has antiviral activity by sensing the presence of dsDNA from DNA viruses in the cytoplasm. Also acts as an innate immune sensor of infection by retroviruses, such as HIV-2, by detecting the presence of reverse-transcribed DNA in the cytosol. In contrast, HIV-1 is poorly sensed by CGAS, due to its capsid that cloaks viral DNA from CGAS detection. Detection of retroviral reverse-transcribed DNA in the cytosol may be indirect and be mediated via interaction with PQBP1, which directly binds reverse-transcribed retroviral DNA. Also detects the presence of DNA from bacteria, such as M.tuberculosis. 2',3'-cGAMP can be transferred from producing cells to neighboring cells through gap junctions, leading to promote STING1 activation and convey immune response to connecting cells. 2',3'-cGAMP can also be transferred between cells by virtue of packaging within viral particles contributing to IFN-induction in newly infected cells in a cGAS-independent but STING1-dependent manner. Also senses the presence of neutrophil extracellular traps (NETs) that are translocated to the cytosol following phagocytosis, leading to synthesis of 2',3'-cGAMP. In addition to foreign DNA, can also be activated by endogenous nuclear or mitochondrial DNA. When self-DNA leaks into the cytosol during cellular stress (such as mitochondrial stress, SARS-CoV-2 infection causing severe COVID-19 disease, DNA damage, mitotic arrest or senescence), or is present in form of cytosolic micronuclei, CGAS is activated leading to a state of sterile inflammation. Acts as a regulator of cellular senescence by binding to cytosolic chromatin fragments that are present in senescent cells, leading to trigger type-I interferon production via STING1 and promote cellular senescence. Also involved in the inflammatory response to genome instability and double-stranded DNA breaks: acts by localizing to micronuclei arising from genome instability. Micronuclei, which are frequently found in cancer cells, consist of chromatin surrounded by their own nuclear membrane: following breakdown of the micronuclear envelope, a process associated with chromothripsis, CGAS binds self-DNA exposed to the cytosol, leading to 2',3'-cGAMP synthesis and subsequent activation of STING1 and type-I interferon production. Activated in response to prolonged mitotic arrest, promoting mitotic cell death. In a healthy cell, CGAS is however kept inactive even in cellular events that directly expose it to self-DNA, such as mitosis, when cGAS associates with chromatin directly after nuclear envelope breakdown or remains in the form of postmitotic persistent nuclear cGAS pools bound to chromatin. Nuclear CGAS is inactivated by chromatin via direct interaction with nucleosomes, which block CGAS from DNA binding and thus prevent CGAS-induced autoimmunity. Also acts as a suppressor of DNA repair in response to DNA damage: inhibits homologous recombination repair by interacting with PARP1, the CGAS-PARP1 interaction leading to impede the formation of the PARP1-TIMELESS complex. In addition to DNA, also sense translation stress: in response to translation stress, translocates to the cytosol and associates with collided ribosomes, promoting its activation and triggering type-I interferon production. In contrast to other mammals, human CGAS displays species-specific mechanisms of DNA recognition and produces less 2',3'-cGAMP, allowing a more fine-tuned response to pathogens.

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 20:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 21:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 12:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 2:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] Sample 33:[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 23:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 13:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 11:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 22:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 27:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 18:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 14:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 9:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 3:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 10:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 17:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 15:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 25:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 29:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 31:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'delta_only', 'wt_only', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 1 samples
  wt_only: 2 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5105

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5045
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing wt_only group:
  Wild-type sequences: 2
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: None
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 2 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.2543

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.2545
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1072.0000
  - Min value: -11968.0000
  - Mean value: -0.2142

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1064.0000
  - Min value: -11904.0000
  - Mean value: -0.2144
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'delta_only', 'delta_only', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 2 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2938

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2954
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11968.0000
  - Mean value: -0.1317

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11904.0000
  - Mean value: -0.1342
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11712.0000
  - Mean value: -0.0977

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11648.0000
  - Mean value: -0.0987
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'wt_only', 'wt_only', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 0 samples
  wt_only: 3 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 120, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1272.0000
  - Min value: -9408.0000
  - Mean value: 0.0045

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 120, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1280.0000
  - Min value: -9472.0000
  - Mean value: -0.0111
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing wt_only group:
  Wild-type sequences: 3
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: -0.0075
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: None
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 2:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([3, 1551, 4096])
  attention_mask: torch.Size([3, 1551])
  labels: torch.Size([3, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])
  Padded shapes:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'delta_only', 'full', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 2 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -12160.0000
  - Mean value: -0.1618

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1639
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'delta_only', 'full', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 2 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -12160.0000
  - Mean value: -0.1618

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1639
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 2 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -10752.0000
  - Mean value: -0.0736

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10880.0000
  - Mean value: -0.0766
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -11968.0000
  - Mean value: -0.1569

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -11904.0000
  - Mean value: -0.1603
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'wt_only', 'full', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 0 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -12160.0000
  - Mean value: -0.1813

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1819
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([1, 1536])

Sequence inputs:
  wild_type_sequences: 1
  mutation_sequences: 1
  attention_mode: ['wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 0 samples
  delta_only: 0 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: -0.0075
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1551
  Number of parts to process: 1

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Part 0 already at max length (1551)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1551])

Shapes after concatenation:
  Inputs embeds: torch.Size([1, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([1, 1551]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([1, 1551])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([1, 1551])
  inputs_embeds: torch.Size([1, 1551, 4096])
  labels: torch.Size([1, 1551])
=== End Forward Pass Debug ===
{'loss': 2.2133, 'learning_rate': 1e-05, 'epoch': 2.0}
[DEBUG] Sample 34:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 3:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 19:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 28:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 27:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 0:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Aspartic acid to Alanine at position 95, the original function of the non-mutated protein is as follows: Nucleotidyltransferase that catalyzes the formation of cyclic GMP-AMP (2',3'-cGAMP) from ATP and GTP and plays a key role in innate immunity. Catalysis involves both the formation of a 2',5' phosphodiester linkage at the GpA step and the formation of a 3',5' phosphodiester linkage at the ApG step, producing c[G(2',5')pA(3',5')p]. Acts as a key DNA sensor: directly binds double-stranded DNA (dsDNA), inducing the formation of liquid-like droplets in which CGAS is activated, leading to synthesis of 2',3'-cGAMP, a second messenger that binds to and activates STING1, thereby triggering type-I interferon production. Preferentially recognizes and binds curved long dsDNAs of a minimal length of 40 bp. Acts as a key foreign DNA sensor, the presence of double-stranded DNA (dsDNA) in the cytoplasm being a danger signal that triggers the immune responses. Has antiviral activity by sensing the presence of dsDNA from DNA viruses in the cytoplasm. Also acts as an innate immune sensor of infection by retroviruses, such as HIV-2, by detecting the presence of reverse-transcribed DNA in the cytosol. In contrast, HIV-1 is poorly sensed by CGAS, due to its capsid that cloaks viral DNA from CGAS detection. Detection of retroviral reverse-transcribed DNA in the cytosol may be indirect and be mediated via interaction with PQBP1, which directly binds reverse-transcribed retroviral DNA. Also detects the presence of DNA from bacteria, such as M.tuberculosis. 2',3'-cGAMP can be transferred from producing cells to neighboring cells through gap junctions, leading to promote STING1 activation and convey immune response to connecting cells. 2',3'-cGAMP can also be transferred between cells by virtue of packaging within viral particles contributing to IFN-induction in newly infected cells in a cGAS-independent but STING1-dependent manner. Also senses the presence of neutrophil extracellular traps (NETs) that are translocated to the cytosol following phagocytosis, leading to synthesis of 2',3'-cGAMP. In addition to foreign DNA, can also be activated by endogenous nuclear or mitochondrial DNA. When self-DNA leaks into the cytosol during cellular stress (such as mitochondrial stress, SARS-CoV-2 infection causing severe COVID-19 disease, DNA damage, mitotic arrest or senescence), or is present in form of cytosolic micronuclei, CGAS is activated leading to a state of sterile inflammation. Acts as a regulator of cellular senescence by binding to cytosolic chromatin fragments that are present in senescent cells, leading to trigger type-I interferon production via STING1 and promote cellular senescence. Also involved in the inflammatory response to genome instability and double-stranded DNA breaks: acts by localizing to micronuclei arising from genome instability. Micronuclei, which are frequently found in cancer cells, consist of chromatin surrounded by their own nuclear membrane: following breakdown of the micronuclear envelope, a process associated with chromothripsis, CGAS binds self-DNA exposed to the cytosol, leading to 2',3'-cGAMP synthesis and subsequent activation of STING1 and type-I interferon production. Activated in response to prolonged mitotic arrest, promoting mitotic cell death. In a healthy cell, CGAS is however kept inactive even in cellular events that directly expose it to self-DNA, such as mitosis, when cGAS associates with chromatin directly after nuclear envelope breakdown or remains in the form of postmitotic persistent nuclear cGAS pools bound to chromatin. Nuclear CGAS is inactivated by chromatin via direct interaction with nucleosomes, which block CGAS from DNA binding and thus prevent CGAS-induced autoimmunity. Also acts as a suppressor of DNA repair in response to DNA damage: inhibits homologous recombination repair by interacting with PARP1, the CGAS-PARP1 interaction leading to impede the formation of the PARP1-TIMELESS complex. In addition to DNA, also sense translation stress: in response to translation stress, translocates to the cytosol and associates with collided ribosomes, promoting its activation and triggering type-I interferon production. In contrast to other mammals, human CGAS displays species-specific mechanisms of DNA recognition and produces less 2',3'-cGAMP, allowing a more fine-tuned response to pathogens.
[DEBUG] Sample 7:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 20:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 11:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 21:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 24:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 9:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 36:[DEBUG] Sample 35:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?

  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 14:
  - has_delta: True[DEBUG] Sample 12:

  - has_wt: True  - has_delta: True

[DEBUG] attention_mode: full  - has_wt: False

[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.[DEBUG] Sample 32:

  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 10:[DEBUG] Sample 2:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 15:[DEBUG] Sample 23:

  - has_delta: True  - has_delta: True
  - has_wt: True[DEBUG] Sample 26:

[DEBUG] attention_mode: full
[DEBUG] Sample 33:[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.

  - has_wt: False  - has_delta: False
  - has_wt: True

[DEBUG] attention_mode: delta_only[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?

[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 1:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 31:[DEBUG] Sample 6:

  - has_delta: True  - has_delta: True

  - has_wt: True  - has_wt: False

[DEBUG] attention_mode: full[DEBUG] attention_mode: delta_only

[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 16:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.[DEBUG] Sample 8:

  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 5:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 29:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 18:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 4:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 13:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 22:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 25:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
=== Forward Pass Debug ===

Input shapes and types:
  input_ids: torch.Size([4, 1536])
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.  attention_mask: torch.Size([4, 1536])

  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'wt_only', 'wt_only', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 1 samples
  wt_only: 2 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
[DEBUG] Sample 30:  Sub-batch shapes:

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?    input_ids: torch.Size([1, 1536])

    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1
[DEBUG] Sample 17:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5105

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5045
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing wt_only group:
  Wild-type sequences: 2
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: -0.0075
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: None
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'delta_only', 'full', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 3 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5105

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5045
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing delta_only group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -11968.0000
  - Mean value: -0.1210

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -11904.0000
  - Mean value: -0.1248
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 0

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([3, 1551, 4096])
  attention_mask: torch.Size([3, 1551])
  labels: torch.Size([3, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])
  Padded shapes:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'full', 'full', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 1 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1272.0000
  - Min value: -11712.0000
  - Mean value: -0.0033

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1280.0000
  - Min value: -11648.0000
  - Mean value: -0.0065
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1072.0000
  - Min value: -11968.0000
  - Mean value: -0.2142

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1064.0000
  - Min value: -11904.0000
  - Mean value: -0.2144
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: -0.0075
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'wt_only', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 1 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -10752.0000
  - Mean value: -0.1480

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10880.0000
  - Mean value: -0.1513
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -10752.0000
  - Mean value: -0.0488

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10880.0000
  - Mean value: -0.0517
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'wt_only', 'full', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 0 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1954

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1980
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'wt_only', 'wt_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 1 samples
  wt_only: 2 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2938

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2954
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing wt_only group:
  Wild-type sequences: 2
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: 0.0156
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: None
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'delta_only', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 3 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5105

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5045
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing delta_only group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11968.0000
  - Mean value: -0.1592

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11904.0000
  - Mean value: -0.1609
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 0

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([3, 1551, 4096])
  attention_mask: torch.Size([3, 1551])
  labels: torch.Size([3, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])
  Padded shapes:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 1 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -12160.0000
  - Mean value: -0.1618

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1639
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([1, 1536])

Sequence inputs:
  wild_type_sequences: 1
  mutation_sequences: 1
  attention_mode: ['full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 0 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5105

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5045
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 1

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([1, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([1, 1566])
  inputs_embeds: torch.Size([1, 1566, 4096])
  labels: torch.Size([1, 1566])
=== End Forward Pass Debug ===
{'loss': 1.8295, 'learning_rate': 1e-05, 'epoch': 3.0}
[DEBUG] Sample 10:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 26:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 32:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 22:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 27:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 21:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 23:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 20:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 4:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 13:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 31:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 34:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 8:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 24:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 30:[DEBUG] Sample 12:[DEBUG] Sample 33:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only

[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?  - has_delta: True

  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 19:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 25:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 17:[DEBUG] Sample 29:

[DEBUG] Sample 5:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 35:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 2:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 28:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 16:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 1:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 11:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 18:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 9:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 36:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 15:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 0:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Aspartic acid to Alanine at position 95, the original function of the non-mutated protein is as follows: Nucleotidyltransferase that catalyzes the formation of cyclic GMP-AMP (2',3'-cGAMP) from ATP and GTP and plays a key role in innate immunity. Catalysis involves both the formation of a 2',5' phosphodiester linkage at the GpA step and the formation of a 3',5' phosphodiester linkage at the ApG step, producing c[G(2',5')pA(3',5')p]. Acts as a key DNA sensor: directly binds double-stranded DNA (dsDNA), inducing the formation of liquid-like droplets in which CGAS is activated, leading to synthesis of 2',3'-cGAMP, a second messenger that binds to and activates STING1, thereby triggering type-I interferon production. Preferentially recognizes and binds curved long dsDNAs of a minimal length of 40 bp. Acts as a key foreign DNA sensor, the presence of double-stranded DNA (dsDNA) in the cytoplasm being a danger signal that triggers the immune responses. Has antiviral activity by sensing the presence of dsDNA from DNA viruses in the cytoplasm. Also acts as an innate immune sensor of infection by retroviruses, such as HIV-2, by detecting the presence of reverse-transcribed DNA in the cytosol. In contrast, HIV-1 is poorly sensed by CGAS, due to its capsid that cloaks viral DNA from CGAS detection. Detection of retroviral reverse-transcribed DNA in the cytosol may be indirect and be mediated via interaction with PQBP1, which directly binds reverse-transcribed retroviral DNA. Also detects the presence of DNA from bacteria, such as M.tuberculosis. 2',3'-cGAMP can be transferred from producing cells to neighboring cells through gap junctions, leading to promote STING1 activation and convey immune response to connecting cells. 2',3'-cGAMP can also be transferred between cells by virtue of packaging within viral particles contributing to IFN-induction in newly infected cells in a cGAS-independent but STING1-dependent manner. Also senses the presence of neutrophil extracellular traps (NETs) that are translocated to the cytosol following phagocytosis, leading to synthesis of 2',3'-cGAMP. In addition to foreign DNA, can also be activated by endogenous nuclear or mitochondrial DNA. When self-DNA leaks into the cytosol during cellular stress (such as mitochondrial stress, SARS-CoV-2 infection causing severe COVID-19 disease, DNA damage, mitotic arrest or senescence), or is present in form of cytosolic micronuclei, CGAS is activated leading to a state of sterile inflammation. Acts as a regulator of cellular senescence by binding to cytosolic chromatin fragments that are present in senescent cells, leading to trigger type-I interferon production via STING1 and promote cellular senescence. Also involved in the inflammatory response to genome instability and double-stranded DNA breaks: acts by localizing to micronuclei arising from genome instability. Micronuclei, which are frequently found in cancer cells, consist of chromatin surrounded by their own nuclear membrane: following breakdown of the micronuclear envelope, a process associated with chromothripsis, CGAS binds self-DNA exposed to the cytosol, leading to 2',3'-cGAMP synthesis and subsequent activation of STING1 and type-I interferon production. Activated in response to prolonged mitotic arrest, promoting mitotic cell death. In a healthy cell, CGAS is however kept inactive even in cellular events that directly expose it to self-DNA, such as mitosis, when cGAS associates with chromatin directly after nuclear envelope breakdown or remains in the form of postmitotic persistent nuclear cGAS pools bound to chromatin. Nuclear CGAS is inactivated by chromatin via direct interaction with nucleosomes, which block CGAS from DNA binding and thus prevent CGAS-induced autoimmunity. Also acts as a suppressor of DNA repair in response to DNA damage: inhibits homologous recombination repair by interacting with PARP1, the CGAS-PARP1 interaction leading to impede the formation of the PARP1-TIMELESS complex. In addition to DNA, also sense translation stress: in response to translation stress, translocates to the cytosol and associates with collided ribosomes, promoting its activation and triggering type-I interferon production. In contrast to other mammals, human CGAS displays species-specific mechanisms of DNA recognition and produces less 2',3'-cGAMP, allowing a more fine-tuned response to pathogens.

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'delta_only', 'wt_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 2 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1
[DEBUG] Sample 3:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 7:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 6:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 14:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2938

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2954
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -11968.0000
  - Mean value: -0.1569

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -11904.0000
  - Mean value: -0.1603
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: -0.0075
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['wt_only', 'full', 'full', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 1 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2938

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2954
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'full', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 0 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.0970

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1006
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -11264.0000
  - Mean value: -0.1963

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -11264.0000
  - Mean value: -0.1957
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1072.0000
  - Min value: -11968.0000
  - Mean value: -0.2142

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1064.0000
  - Min value: -11904.0000
  - Mean value: -0.2144
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'full', 'full', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11712.0000
  - Mean value: -0.0977

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11648.0000
  - Mean value: -0.0987
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'wt_only', 'wt_only', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 0 samples
  delta_only: 2 samples
  wt_only: 2 samples
  text_only: 0 samples

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -11968.0000
  - Mean value: -0.1569

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -11904.0000
  - Mean value: -0.1603
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing wt_only group:
  Wild-type sequences: 2
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: 0.0156
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: None
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1551
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Part 0 already at max length (1551)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Part 1 already at max length (1551)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1551])
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1551])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1551]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1551])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1551, 4096])
  attention_mask: torch.Size([4, 1551])
  labels: torch.Size([4, 1551])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1551])
  inputs_embeds: torch.Size([4, 1551, 4096])
  labels: torch.Size([4, 1551])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'wt_only', 'wt_only', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 0 samples
  delta_only: 1 samples
  wt_only: 3 samples
  text_only: 0 samples

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing wt_only group:
  Wild-type sequences: 3
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: 0.0079
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: None
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 2:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([3, 1551, 4096])
  attention_mask: torch.Size([3, 1551])
  labels: torch.Size([3, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1551
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Part 0 already at max length (1551)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])

Part 1 already at max length (1551)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1551])
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1551])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1551]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1551])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1551, 4096])
  attention_mask: torch.Size([4, 1551])
  labels: torch.Size([4, 1551])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1551])
  inputs_embeds: torch.Size([4, 1551, 4096])
  labels: torch.Size([4, 1551])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'full', 'full', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -10752.0000
  - Mean value: -0.1480

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10880.0000
  - Mean value: -0.1513
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1072.0000
  - Min value: -11968.0000
  - Mean value: -0.2142

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1064.0000
  - Min value: -11904.0000
  - Mean value: -0.2144
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'full', 'delta_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 2 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.2543

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.2545
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.1241

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.1336
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([1, 1536])

Sequence inputs:
  wild_type_sequences: 1
  mutation_sequences: 1
  attention_mode: ['delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 0 samples
  delta_only: 1 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0820

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -10368.0000
  - Mean value: -0.0900
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1551
  Number of parts to process: 1

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Part 0 already at max length (1551)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1551])

Shapes after concatenation:
  Inputs embeds: torch.Size([1, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([1, 1551]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([1, 1551])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([1, 1551])
  inputs_embeds: torch.Size([1, 1551, 4096])
  labels: torch.Size([1, 1551])
=== End Forward Pass Debug ===
{'loss': 1.5676, 'learning_rate': 1e-05, 'epoch': 4.0}
[DEBUG] Sample 29:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 16:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 1:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 18:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 9:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 17:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 33:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 20:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 27:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.[DEBUG] Sample 2:

  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 36:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Leucine to Lysine at position 264, the original function of the non-mutated protein is as follows: Chemoreceptor that plays a critical role in the virulence and pathogenesis of P.aeruginosa in a variety of hosts. Probably acts through oxygen sensing. Uses a heme-based sensor. Could be involved in chemotaxis. When expressed in E.coli, is able to sense and mediate repellent responses to oxygen, carbon monoxide and nitric oxide.
[DEBUG] Sample 26:[DEBUG] Sample 12:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 34:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.

  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 30:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 3:[DEBUG] Sample 23:
  - has_delta: True

  - has_wt: True  - has_delta: True

[DEBUG] attention_mode: full  - has_wt: False
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.

[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 35:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 11:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 13:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 5:
[DEBUG] Sample 25:  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 24:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 21:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 22:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 32:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 31:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 7:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Lysine to Serine at position 6, the original function of the non-mutated protein is as follows: Controls the reversible conversion of biologically active glucocorticoids such as cortisone to cortisol, and 11-dehydrocorticosterone to corticosterone in the presence of NADP(H). Participates in the corticosteroid receptor-mediated anti-inflammatory response, as well as metabolic and homeostatic processes. Plays a role in the secretion of aqueous humor in the eye, maintaining a normotensive, intraocular environment. Bidirectional in vitro, predominantly functions as a reductase in vivo, thereby increasing the concentration of active glucocorticoids. It has broad substrate specificity, besides glucocorticoids, it accepts other steroid and sterol substrates. Interconverts 7-oxo- and 7-hydroxy-neurosteroids such as 7-oxopregnenolone and 7beta-hydroxypregnenolone, 7-oxodehydroepiandrosterone (3beta-hydroxy-5-androstene-7,17-dione) and 7beta-hydroxydehydroepiandrosterone (3beta,7beta-dihydroxyandrost-5-en-17-one), among others. Catalyzes the stereo-specific conversion of the major dietary oxysterol, 7-ketocholesterol (7-oxocholesterol), into the more polar 7-beta-hydroxycholesterol metabolite. 7-oxocholesterol is one of the most important oxysterols, it participates in several events such as induction of apoptosis, accumulation in atherosclerotic lesions, lipid peroxidation, and induction of foam cell formation. Mediates the 7-oxo reduction of 7-oxolithocholate mainly to chenodeoxycholate, and to a lesser extent to ursodeoxycholate, both in its free form and when conjugated to glycine or taurine, providing a link between glucocorticoid activation and bile acid metabolism. Catalyzes the synthesis of 7-beta-25-dihydroxycholesterol from 7-oxo-25-hydroxycholesterol in vitro, which acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration.
[DEBUG] Sample 14:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Cysteine to Serine at position 4, the original function of the non-mutated protein is as follows: Metal-responsive transcriptional repressor for the cmt operon. Binding of cadmium or lead causes the repressor to dissociate from the DNA.
[DEBUG] Sample 8:
  - has_delta: False
  - has_wt: True
[DEBUG] attention_mode: wt_only
[DEBUG] human_query: <wt_protein> </wt_protein> Describe this protein?
[DEBUG] Sample 15:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 28:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 10:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 4:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Arginine to Cysteine at position 337, the original function of the non-mutated protein is as follows: Catalyzes the conversion of biologically active 11beta-hydroxyglucocorticoids (11beta-hydroxysteroid) such as cortisol, to inactive 11-ketoglucocorticoids (11-oxosteroid) such as cortisone, in the presence of NAD(+). Functions as a dehydrogenase (oxidase), thereby decreasing the concentration of active glucocorticoids, thus protecting the nonselective mineralocorticoid receptor from occupation by glucocorticoids. Plays an important role in maintaining glucocorticoids balance during preimplantation and protects the fetus from excessive maternal corticosterone exposure. Catalyzes the oxidation of 11beta-hydroxytestosterone (11beta,17beta-dihydroxyandrost-4-ene-3-one) to 11-ketotestosterone (17beta-hydroxyandrost-4-ene-3,11-dione), a major bioactive androgen. Catalyzes the conversion of 11beta-hydroxyandrostenedione (11beta-hydroxyandrost-4-ene-3,17-dione) to 11-ketoandrostenedione (androst-4-ene-3,11,17-trione), which can be further metabolized to 11-ketotestosterone. Converts 7-beta-25-dihydroxycholesterol to 7-oxo-25-hydroxycholesterol in vitro. 7-beta-25-dihydroxycholesterol (not 7-oxo-25-hydroxycholesterol) acts as ligand for the G-protein-coupled receptor (GPCR) Epstein-Barr virus-induced gene 2 (EBI2) and may thereby regulate immune cell migration. May protect ovulating oocytes and fertilizing spermatozoa from the adverse effects of cortisol.
[DEBUG] Sample 6:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?
[DEBUG] Sample 0:
  - has_delta: True
  - has_wt: True
[DEBUG] attention_mode: full
[DEBUG] human_query: Describe the effect of this mutation ' <mut_protein> </mut_protein> ' on this protein ' <wt_protein> </wt_protein> '. Context: There is a change of amino acid Aspartic acid to Alanine at position 95, the original function of the non-mutated protein is as follows: Nucleotidyltransferase that catalyzes the formation of cyclic GMP-AMP (2',3'-cGAMP) from ATP and GTP and plays a key role in innate immunity. Catalysis involves both the formation of a 2',5' phosphodiester linkage at the GpA step and the formation of a 3',5' phosphodiester linkage at the ApG step, producing c[G(2',5')pA(3',5')p]. Acts as a key DNA sensor: directly binds double-stranded DNA (dsDNA), inducing the formation of liquid-like droplets in which CGAS is activated, leading to synthesis of 2',3'-cGAMP, a second messenger that binds to and activates STING1, thereby triggering type-I interferon production. Preferentially recognizes and binds curved long dsDNAs of a minimal length of 40 bp. Acts as a key foreign DNA sensor, the presence of double-stranded DNA (dsDNA) in the cytoplasm being a danger signal that triggers the immune responses. Has antiviral activity by sensing the presence of dsDNA from DNA viruses in the cytoplasm. Also acts as an innate immune sensor of infection by retroviruses, such as HIV-2, by detecting the presence of reverse-transcribed DNA in the cytosol. In contrast, HIV-1 is poorly sensed by CGAS, due to its capsid that cloaks viral DNA from CGAS detection. Detection of retroviral reverse-transcribed DNA in the cytosol may be indirect and be mediated via interaction with PQBP1, which directly binds reverse-transcribed retroviral DNA. Also detects the presence of DNA from bacteria, such as M.tuberculosis. 2',3'-cGAMP can be transferred from producing cells to neighboring cells through gap junctions, leading to promote STING1 activation and convey immune response to connecting cells. 2',3'-cGAMP can also be transferred between cells by virtue of packaging within viral particles contributing to IFN-induction in newly infected cells in a cGAS-independent but STING1-dependent manner. Also senses the presence of neutrophil extracellular traps (NETs) that are translocated to the cytosol following phagocytosis, leading to synthesis of 2',3'-cGAMP. In addition to foreign DNA, can also be activated by endogenous nuclear or mitochondrial DNA. When self-DNA leaks into the cytosol during cellular stress (such as mitochondrial stress, SARS-CoV-2 infection causing severe COVID-19 disease, DNA damage, mitotic arrest or senescence), or is present in form of cytosolic micronuclei, CGAS is activated leading to a state of sterile inflammation. Acts as a regulator of cellular senescence by binding to cytosolic chromatin fragments that are present in senescent cells, leading to trigger type-I interferon production via STING1 and promote cellular senescence. Also involved in the inflammatory response to genome instability and double-stranded DNA breaks: acts by localizing to micronuclei arising from genome instability. Micronuclei, which are frequently found in cancer cells, consist of chromatin surrounded by their own nuclear membrane: following breakdown of the micronuclear envelope, a process associated with chromothripsis, CGAS binds self-DNA exposed to the cytosol, leading to 2',3'-cGAMP synthesis and subsequent activation of STING1 and type-I interferon production. Activated in response to prolonged mitotic arrest, promoting mitotic cell death. In a healthy cell, CGAS is however kept inactive even in cellular events that directly expose it to self-DNA, such as mitosis, when cGAS associates with chromatin directly after nuclear envelope breakdown or remains in the form of postmitotic persistent nuclear cGAS pools bound to chromatin. Nuclear CGAS is inactivated by chromatin via direct interaction with nucleosomes, which block CGAS from DNA binding and thus prevent CGAS-induced autoimmunity. Also acts as a suppressor of DNA repair in response to DNA damage: inhibits homologous recombination repair by interacting with PARP1, the CGAS-PARP1 interaction leading to impede the formation of the PARP1-TIMELESS complex. In addition to DNA, also sense translation stress: in response to translation stress, translocates to the cytosol and associates with collided ribosomes, promoting its activation and triggering type-I interferon production. In contrast to other mammals, human CGAS displays species-specific mechanisms of DNA recognition and produces less 2',3'-cGAMP, allowing a more fine-tuned response to pathogens.

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['wt_only', 'full', 'wt_only', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 0 samples
  wt_only: 2 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2
[DEBUG] Sample 19:
  - has_delta: True
  - has_wt: False
[DEBUG] attention_mode: delta_only
[DEBUG] human_query: <mut_protein> </mut_protein> Describe the effect of the mutation?

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -10752.0000
  - Mean value: -0.1480

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10880.0000
  - Mean value: -0.1513
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing wt_only group:
  Wild-type sequences: 2
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: -0.0075
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: None
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'full', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 3 samples
  delta_only: 0 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing full group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -11264.0000
  - Mean value: -0.1963

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -11264.0000
  - Mean value: -0.1957
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: torch.Size([3, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: torch.Size([3, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 3

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([3, 1566, 4096])
  attention_mask: torch.Size([3, 1566])
  labels: torch.Size([3, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'wt_only', 'delta_only', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 0 samples
  delta_only: 2 samples
  wt_only: 2 samples
  text_only: 0 samples

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11968.0000
  - Mean value: -0.1317

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11904.0000
  - Mean value: -0.1342
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing wt_only group:
  Wild-type sequences: 2
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: 0.0156
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: None
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Processing batch item 1:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1551
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Part 0 already at max length (1551)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Part 1 already at max length (1551)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1551])
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1551]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1551])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1551, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1551]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1551])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1551, 4096])
  attention_mask: torch.Size([4, 1551])
  labels: torch.Size([4, 1551])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1551])
  inputs_embeds: torch.Size([4, 1551, 4096])
  labels: torch.Size([4, 1551])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'delta_only', 'full', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 1 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1440.0000
  - Min value: -10752.0000
  - Mean value: -0.0736

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 681, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10880.0000
  - Mean value: -0.0766
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 356, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1232.0000
  - Min value: -10240.0000
  - Mean value: -0.0075
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'full', 'full', 'wt_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 1 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1462

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1493
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['full', 'full', 'delta_only', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 2 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5105

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 294, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1296.0000
  - Min value: -11264.0000
  - Mean value: -0.5045
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11968.0000
  - Mean value: -0.1317

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11904.0000
  - Mean value: -0.1342
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['wt_only', 'delta_only', 'full', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 1 samples
  wt_only: 1 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1462

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1493
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing delta_only group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.2663

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 226, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.2838
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 0

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing wt_only group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing wt_only group:
  Wild-type sequences: 1
  Mutation sequences: 0

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 94, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 960.0000
  - Min value: -8704.0000
  - Mean value: 0.1464
  GCA+Resampler output shapes:
    delta_features: None
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: None
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 0
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [14]
  Found delta tokens at: []
  Processing wt_only mode
  Merging wildtype features at index 14
  Number of wildtype tokens: 16
  Merged embeddings shape: torch.Size([1551, 4096])

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([1, 1551, 4096])
  attention_mask: torch.Size([1, 1551])
  labels: torch.Size([1, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 3

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part shapes before padding:
  Part 2:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])

Padding part 2:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([1, 1551, 4096])
    Attention mask: torch.Size([1, 1551])
    Labels: torch.Size([1, 1551])
  Padded shapes:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 2:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'delta_only', 'full', 'full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 2 samples
  delta_only: 2 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing full group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1462

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.1493
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: torch.Size([2, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: torch.Size([2, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 2

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([2, 1566, 4096])
  attention_mask: torch.Size([2, 1566])
  labels: torch.Size([2, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 2 samples
  Sub-batch shapes:
    input_ids: torch.Size([2, 1536])
    attention_mask: torch.Size([2, 1536])
    labels: torch.Size([2, 1536])

Processing delta_only group:
  Wild-type sequences: 2
  Mutation sequences: 2

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([2, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1408.0000
  - Min value: -10368.0000
  - Mean value: -0.1241

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([2, 362, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -10368.0000
  - Mean value: -0.1336
  GCA+Resampler output shapes:
    delta_features: torch.Size([2, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([2, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([2, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([2, 1536])
  attention_mask: torch.Size([2, 1536])
  labels: torch.Size([2, 1536])
  delta_features: torch.Size([2, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 2
  Wildtype tokens: 0

Token embeddings shape: torch.Size([2, 1536, 4096])
Batch size: 2, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([2, 1551, 4096])
  attention_mask: torch.Size([2, 1551])
  labels: torch.Size([2, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([2, 1551, 4096])
    Attention mask: torch.Size([2, 1551])
    Labels: torch.Size([2, 1551])
  Padded shapes:
    Embeddings: torch.Size([2, 1566, 4096])
    Attention mask: torch.Size([2, 1566])
    Labels: torch.Size([2, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])
  Part 1:
    Embeddings: torch.Size([2, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([2, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([2, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([4, 1536])
  attention_mask: torch.Size([4, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([4, 1536])

Sequence inputs:
  wild_type_sequences: 4
  mutation_sequences: 4
  attention_mode: ['delta_only', 'delta_only', 'full', 'delta_only']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 3 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1264.0000
  - Min value: -11712.0000
  - Mean value: -0.0077

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 524, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1264.0000
  - Min value: -11648.0000
  - Mean value: -0.0105
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

[DEBUG] Processing delta_only group with 3 samples
  Sub-batch shapes:
    input_ids: torch.Size([3, 1536])
    attention_mask: torch.Size([3, 1536])
    labels: torch.Size([3, 1536])

Processing delta_only group:
  Wild-type sequences: 3
  Mutation sequences: 3

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([3, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11968.0000
  - Mean value: -0.1592

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([3, 604, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1136.0000
  - Min value: -11904.0000
  - Mean value: -0.1609
  GCA+Resampler output shapes:
    delta_features: torch.Size([3, 16, 1536])
    wt_features: None

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([3, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([3, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([3, 1536])
  attention_mask: torch.Size([3, 1536])
  labels: torch.Size([3, 1536])
  delta_features: torch.Size([3, 16, 4096])
  wt_features: None

Special token counts in input_ids:
  Delta tokens: 3
  Wildtype tokens: 0

Token embeddings shape: torch.Size([3, 1536, 4096])
Batch size: 3, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 1:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Processing batch item 2:
  Found wildtype tokens at: []
  Found delta tokens at: [14]
  Processing delta or full mode
  Delta token index: 14
  Number of protein tokens: 16

Padding all sequences to max length: 1551

Final output shapes:
  input_embeds: torch.Size([3, 1551, 4096])
  attention_mask: torch.Size([3, 1551])
  labels: torch.Size([3, 1551])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 2

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Part shapes before padding:
  Part 1:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])

Padding part 1:
  Current length: 1551
  Target length: 1566
  Padding length: 15
  Original shapes:
    Embeddings: torch.Size([3, 1551, 4096])
    Attention mask: torch.Size([3, 1551])
    Labels: torch.Size([3, 1551])
  Padded shapes:
    Embeddings: torch.Size([3, 1566, 4096])
    Attention mask: torch.Size([3, 1566])
    Labels: torch.Size([3, 1566])

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])
  Part 1:
    Embeddings: torch.Size([3, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([3, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([3, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([4, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([4, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([4, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([4, 1566, 4096])
  attention_mask: torch.Size([4, 1566])
  labels: torch.Size([4, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([4, 1566])
  inputs_embeds: torch.Size([4, 1566, 4096])
  labels: torch.Size([4, 1566])
=== End Forward Pass Debug ===

=== Forward Pass Debug ===
Input shapes and types:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  position_ids: None
  inputs_embeds: None
  labels: torch.Size([1, 1536])

Sequence inputs:
  wild_type_sequences: 1
  mutation_sequences: 1
  attention_mode: ['full']

Has protein input: True

Grouping samples by attention_mode...
Grouped indices:
  full: 1 samples
  delta_only: 0 samples
  wt_only: 0 samples
  text_only: 0 samples

[DEBUG] Processing full group with 1 samples
  Sub-batch shapes:
    input_ids: torch.Size([1, 1536])
    attention_mask: torch.Size([1, 1536])
    labels: torch.Size([1, 1536])

Processing full group:
  Wild-type sequences: 1
  Mutation sequences: 1

[DEBUG] ESM Encoder Output (Wild-Type):
  - Shape: torch.Size([1, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2938

[DEBUG] ESM Encoder Output (Mutation):
  - Shape: torch.Size([1, 407, 1536])
  - Dtype: torch.float32
  - Contains NaN: False
  - Contains Inf: False
  - Max value: 1424.0000
  - Min value: -12160.0000
  - Mean value: -0.2954
  GCA+Resampler output shapes:
    delta_features: torch.Size([1, 16, 1536])
    wt_features: torch.Size([1, 16, 1536])

[DEBUG] MLPProjector forward (delta):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection: torch.Size([1, 16, 4096])

[DEBUG] MLPProjector forward (wt):
  Input shape: torch.Size([1, 16, 1536]), dtype: torch.bfloat16
    After projection (wt): torch.Size([1, 16, 4096])

=== prepare_inputs_labels_for_multimodal Debug ===
Input shapes:
  input_ids: torch.Size([1, 1536])
  attention_mask: torch.Size([1, 1536])
  labels: torch.Size([1, 1536])
  delta_features: torch.Size([1, 16, 4096])
  wt_features: torch.Size([1, 16, 4096])

Special token counts in input_ids:
  Delta tokens: 1
  Wildtype tokens: 1

Token embeddings shape: torch.Size([1, 1536, 4096])
Batch size: 1, Hidden dim: 4096

Processing batch item 0:
  Found wildtype tokens at: [30]
  Found delta tokens at: [21]
  Processing delta or full mode
  Delta token index: 21
  Number of protein tokens: 16

Padding all sequences to max length: 1566

Final output shapes:
  input_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])
=== End prepare_inputs_labels_for_multimodal Debug ===

Combining processed parts...

Padding details:
  Max sequence length: 1566
  Number of parts to process: 1

Part shapes before padding:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096])
    Attention mask: torch.Size([1, 1566])
    Labels: torch.Size([1, 1566])

Part 0 already at max length (1566)

Concatenating padded parts:
Shapes before concatenation:
  Part 0:
    Embeddings: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
    Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
    Labels: torch.Size([1, 1566])

Shapes after concatenation:
  Inputs embeds: torch.Size([1, 1566, 4096]), dtype: torch.bfloat16, device: cuda:0
  Attention mask: torch.Size([1, 1566]), dtype: torch.int64, device: cuda:0
  Labels: torch.Size([1, 1566])

Reordering to original batch order...
Final reordered shapes:
  inputs_embeds: torch.Size([1, 1566, 4096])
  attention_mask: torch.Size([1, 1566])
  labels: torch.Size([1, 1566])

Forwarding to parent class with shapes:
  input_ids: None
  attention_mask: torch.Size([1, 1566])
  inputs_embeds: torch.Size([1, 1566, 4096])
  labels: torch.Size([1, 1566])
=== End Forward Pass Debug ===
{'loss': 1.2623, 'learning_rate': 1e-05, 'epoch': 5.0}
{'train_runtime': 134.4634, 'train_samples_per_second': 1.376, 'train_steps_per_second': 0.372, 'train_loss': 1.877143020629883, 'epoch': 5.0}
[TIME] trainer.train() took 152.75 seconds.
[DEBUG] trainer.train() finished. Result: TrainOutput(global_step=50, training_loss=1.877143020629883, metrics={'train_runtime': 134.4634, 'train_samples_per_second': 1.376, 'train_steps_per_second': 0.372, 'train_loss': 1.877143020629883, 'epoch': 5.0})
[DEBUG] Saving model...
[AdapterTrainer] PEFT adapter saved to ./output/finetune_lora
[TIME] trainer.save_model() took 0.55 seconds.
[DEBUG] Logging and saving metrics...
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     1.8771
  train_runtime            = 0:02:14.46
  train_samples_per_second =      1.376
  train_steps_per_second   =      0.372
[TIME] Metrics logging and saving took 0.00 seconds.
LoRA finetuning finished. Model and state saved.
[2025-07-04 21:05:16,282] [INFO] [launch.py:351:main] Process 519873 exits successfully.
LoRA Finetuning finished.
